---
title: "Welcome to all my code"
execute:
  echo: true
  eval: false
---

## Data Cleaning

In this section, we will perform data cleaning and prepare various datasets for analysis.

```{python}
import os
import numpy as np
import torch
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

```

Extract names of all data files, store in `files` array, and extract names of all presidents and store in `president_names` array.

```{python}
#| echo: true

folder_path = 'data'
files = os.listdir(folder_path)
files = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]

for file in files[:3]:
    print(file)

president_names = []

# Define a regular expression pattern to match the president's name
pattern = r'_(.*?)\.txt'

for file in files:
    match = re.search(pattern, file)
    if match:
        president_name = match.group(1)
        # remove prefix and if not there its fine already
        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')
        president_names.append(president_name)


```

Make dataset of presidents (column 1) and sentences (column 2).

```{python}
#| echo: true
df = pd.DataFrame(columns=['completion', 'prompt'])

# read in file and skip the first two lines
for file_index in range(len(files)):

    file_path = f'data/{files[file_index]}'  
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()[2:]

    # Combine the lines into a single text
    text = ' '.join(lines)

    # tokenize the text into sentences using NLTK
    sentences = sent_tokenize(text) # remove "\n" 
    # remove "\n"
    cleaned_sentences = [sentence.replace('\n', '') for sentence in sentences]

    current_president = president_names[file_index]
    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})
    df = pd.concat([df,dftemp], axis=0)

# remove stopwords function
def remove_stopwords(sentence):
    words = sentence.split()
    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]
    return " ".join(filtered_words)

df['prompt'] = df['prompt'].apply(remove_stopwords)

df.reset_index(drop=True, inplace=True)

df.to_csv("data.csv")
```

## Bag of Words 

```{python}
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer

def bow_x():
    # Read the data once
    data = pd.read_csv("data.csv")
    
    # Extract relevant columns
    text_data = data['prompt']
    y = data['completion']
    
    # Initialize a CountVectorizer for BOW representation
    vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b")
    
    # Fit and transform the text data
    X = vectorizer.fit_transform(text_data)
    
    # Create a DataFrame from the BOW representation
    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
    
    return bow_df

# function to return names of presidents
def bow_y():
  y = names['completion']
  return(y)

```

## Term Frequency - Inverse Document Frequency

```{python}
# Imports
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
```

```{python}

def tf_idf():
    df = pd.read_csv("data.csv").iloc[:,1:]
    sentences = df['prompt'].tolist()

    # Create a TfidfVectorizer
    tfidf_vectorizer = TfidfVectorizer()

    # Fit and transform the sentences to compute TF-IDF values
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

    # Create a new dataframe with TF-IDF values
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
```

## Word Embedding

```{python}

import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

def embeddings_data_prep():
    data = pd.read_csv("data.csv").iloc[:, 1:]

    # Tokenization
    max_features = 1000
    tokenizer = Tokenizer(num_words=max_features)
    tokenizer.fit_on_texts(data['prompt'])
    sequences = tokenizer.texts_to_sequences(data['prompt'])

    # Filter out empty sequences and corresponding labels
    filtered_indices = [i for i, s in enumerate(sequences) if len(s) > 0]
    sequences = [sequences[i] for i in filtered_indices]
    y = data['completion'].iloc[filtered_indices].values

    # Splitting data into training, validation, and test sets
    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)
    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)

    maxlen = 50
    x_train_pad = pad_sequences(x_train, maxlen=maxlen)
    x_val_pad = pad_sequences(x_val, maxlen=maxlen)
    x_test_pad = pad_sequences(x_test, maxlen=maxlen)

    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test


#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()
```