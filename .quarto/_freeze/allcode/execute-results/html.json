{
  "hash": "164c550986170434a68be1905a744659",
  "result": {
    "markdown": "---\ntitle: \"Welcome to all my code\"\nexecute:\n  echo: true\n  eval: false\n---\n\n## Data Cleaning\n\nIn this section, we will perform data cleaning and prepare various datasets for analysis.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n```\n:::\n\n\nExtract names of all data files, store in `files` array, and extract names of all presidents and store in `president_names` array.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\n```\n:::\n\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n```\n:::\n\n\n### Bag of Words \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n```\n:::\n\n\n### Term Frequency - Inverse Document Frequency\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n```\n:::\n\n\n## Word Embedding\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef embeddings_data_prep():\n    data = pd.read_csv(\"data.csv\").iloc[:, 1:]\n\n    # Tokenization\n    max_features = 1000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data['prompt'])\n    sequences = tokenizer.texts_to_sequences(data['prompt'])\n\n    # Filter out empty sequences and corresponding labels\n    filtered_indices = [i for i, s in enumerate(sequences) if len(s) > 0]\n    sequences = [sequences[i] for i in filtered_indices]\n    y = data['completion'].iloc[filtered_indices].values\n\n    # Splitting data into training, validation, and test sets\n    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)\n    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n    maxlen = 50\n    x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n    x_val_pad = pad_sequences(x_val, maxlen=maxlen)\n    x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n\n    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test\n\n\n#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()\n```\n:::\n\n\n## Exploratory Data Analysis\n\nNumber of sentences per president.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndata = pd.read_csv(\"data.csv\").iloc[:,1:]\nsentence_counts = data['completion'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=sentence_counts.index, y=sentence_counts.values)\n\nplt.xlabel('Presidents')\nplt.ylabel('Number of Sentences')\nplt.savefig('eda_plots/sentence_counts.png', bbox_inches='tight')\nplt.show()\n```\n:::\n\n\nSentence length plot.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndata['sentence_length'] = data['prompt'].apply(lambda x: len(x.split()))\naverage_sentence_length = data.groupby('completion')['sentence_length'].mean().reset_index()\ndesired_order = [4, 2, 3, 1, 0, 5]\naverage_sentence_length = average_sentence_length.loc[desired_order]\n# Plot the barplot of average sentence lengths per president\nplt.figure(figsize=(10, 6))\nsns.barplot(x='completion', y='sentence_length', data=average_sentence_length)\nplt.xlabel('Presidents')\nplt.ylabel('Average Sentence Length')\nplt.savefig('eda_plots/sentence_length.png', bbox_inches='tight')\nplt.show()\n```\n:::\n\n\nProduce word cloud.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\npresidents = data['completion'].unique()\nfor president in presidents:\n    text = \" \".join(data[data['completion'] == president]['prompt'])\n    \n    # Create a WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='grey').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    #plt.title(f'Word Cloud for President {president}')\n    plt.axis('off')\n    plt.show()\n```\n:::\n\n\n## Models\n### Neural Network\n\n",
    "supporting": [
      "allcode_files"
    ],
    "filters": [],
    "includes": {}
  }
}