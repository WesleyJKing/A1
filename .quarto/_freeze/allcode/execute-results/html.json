{
  "hash": "b40d3a50c14ae8142e7d282728699f65",
  "result": {
    "markdown": "---\ntitle: \"Welcome to all my code\"\nexecute:\n  echo: true\n  eval: false\n---\n\n## Data Cleaning\n\nIn this section, we will perform data cleaning and prepare various datasets for analysis.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n```\n:::\n\n\nExtract names of all data files, store in `files` array, and extract names of all presidents and store in `president_names` array.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\n```\n:::\n\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n```\n:::\n\n\n",
    "supporting": [
      "allcode_files"
    ],
    "filters": [],
    "includes": {}
  }
}