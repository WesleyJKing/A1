{"title":"Welcome to all my code","markdown":{"yaml":{"title":"Welcome to all my code","execute":{"echo":true,"eval":false}},"headingText":"Data Cleaning","containsRefs":false,"markdown":"\n\n\nIn this section, we will perform data cleaning and prepare various datasets for analysis.\n\n```{python}\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n```\n\nExtract names of all data files, store in `files` array, and extract names of all presidents and store in `president_names` array.\n\n```{python}\n#| echo: true\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\n\n```\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\n```{python}\n#| echo: true\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n```\n\n### Bag of Words \n\n```{python}\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n\n```\n\n### Term Frequency - Inverse Document Frequency\n\n```{python}\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```\n\n```{python}\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n```\n\n## Word Embedding\n\n```{python}\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef embeddings_data_prep():\n    data = pd.read_csv(\"data.csv\").iloc[:, 1:]\n\n    # Tokenization\n    max_features = 1000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data['prompt'])\n    sequences = tokenizer.texts_to_sequences(data['prompt'])\n\n    # Filter out empty sequences and corresponding labels\n    filtered_indices = [i for i, s in enumerate(sequences) if len(s) > 0]\n    sequences = [sequences[i] for i in filtered_indices]\n    y = data['completion'].iloc[filtered_indices].values\n\n    # Splitting data into training, validation, and test sets\n    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)\n    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n    maxlen = 50\n    x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n    x_val_pad = pad_sequences(x_val, maxlen=maxlen)\n    x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n\n    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test\n\n\n#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()\n```\n\n## Exploratory Data Analysis\n\nNumber of sentences per president.\n\n```{python}\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndata = pd.read_csv(\"data.csv\").iloc[:,1:]\nsentence_counts = data['completion'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=sentence_counts.index, y=sentence_counts.values)\n\nplt.xlabel('Presidents')\nplt.ylabel('Number of Sentences')\nplt.savefig('eda_plots/sentence_counts.png', bbox_inches='tight')\nplt.show()\n\n```\n\nSentence length plot.\n\n```{python}\ndata['sentence_length'] = data['prompt'].apply(lambda x: len(x.split()))\naverage_sentence_length = data.groupby('completion')['sentence_length'].mean().reset_index()\ndesired_order = [4, 2, 3, 1, 0, 5]\naverage_sentence_length = average_sentence_length.loc[desired_order]\n# Plot the barplot of average sentence lengths per president\nplt.figure(figsize=(10, 6))\nsns.barplot(x='completion', y='sentence_length', data=average_sentence_length)\nplt.xlabel('Presidents')\nplt.ylabel('Average Sentence Length')\nplt.savefig('eda_plots/sentence_length.png', bbox_inches='tight')\nplt.show()\n```\n\nProduce word cloud.\n\n```{python}\n\npresidents = data['completion'].unique()\nfor president in presidents:\n    text = \" \".join(data[data['completion'] == president]['prompt'])\n    \n    # Create a WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='grey').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    #plt.title(f'Word Cloud for President {president}')\n    plt.axis('off')\n    plt.show()\n```\n\n## Models\n### Neural Network\n\n\n\n\n","srcMarkdownNoYaml":"\n\n## Data Cleaning\n\nIn this section, we will perform data cleaning and prepare various datasets for analysis.\n\n```{python}\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n```\n\nExtract names of all data files, store in `files` array, and extract names of all presidents and store in `president_names` array.\n\n```{python}\n#| echo: true\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\n\n```\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\n```{python}\n#| echo: true\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n```\n\n### Bag of Words \n\n```{python}\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n\n```\n\n### Term Frequency - Inverse Document Frequency\n\n```{python}\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```\n\n```{python}\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n```\n\n## Word Embedding\n\n```{python}\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef embeddings_data_prep():\n    data = pd.read_csv(\"data.csv\").iloc[:, 1:]\n\n    # Tokenization\n    max_features = 1000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data['prompt'])\n    sequences = tokenizer.texts_to_sequences(data['prompt'])\n\n    # Filter out empty sequences and corresponding labels\n    filtered_indices = [i for i, s in enumerate(sequences) if len(s) > 0]\n    sequences = [sequences[i] for i in filtered_indices]\n    y = data['completion'].iloc[filtered_indices].values\n\n    # Splitting data into training, validation, and test sets\n    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)\n    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n    maxlen = 50\n    x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n    x_val_pad = pad_sequences(x_val, maxlen=maxlen)\n    x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n\n    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test\n\n\n#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()\n```\n\n## Exploratory Data Analysis\n\nNumber of sentences per president.\n\n```{python}\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndata = pd.read_csv(\"data.csv\").iloc[:,1:]\nsentence_counts = data['completion'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=sentence_counts.index, y=sentence_counts.values)\n\nplt.xlabel('Presidents')\nplt.ylabel('Number of Sentences')\nplt.savefig('eda_plots/sentence_counts.png', bbox_inches='tight')\nplt.show()\n\n```\n\nSentence length plot.\n\n```{python}\ndata['sentence_length'] = data['prompt'].apply(lambda x: len(x.split()))\naverage_sentence_length = data.groupby('completion')['sentence_length'].mean().reset_index()\ndesired_order = [4, 2, 3, 1, 0, 5]\naverage_sentence_length = average_sentence_length.loc[desired_order]\n# Plot the barplot of average sentence lengths per president\nplt.figure(figsize=(10, 6))\nsns.barplot(x='completion', y='sentence_length', data=average_sentence_length)\nplt.xlabel('Presidents')\nplt.ylabel('Average Sentence Length')\nplt.savefig('eda_plots/sentence_length.png', bbox_inches='tight')\nplt.show()\n```\n\nProduce word cloud.\n\n```{python}\n\npresidents = data['completion'].unique()\nfor president in presidents:\n    text = \" \".join(data[data['completion'] == president]['prompt'])\n    \n    # Create a WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='grey').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    #plt.title(f'Word Cloud for President {president}')\n    plt.axis('off')\n    plt.show()\n```\n\n## Models\n### Neural Network\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"allcode.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Welcome to all my code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}