[
  {
    "objectID": "writeup.html",
    "href": "writeup.html",
    "title": "A Quarto Website",
    "section": "",
    "text": "This study explores the challenging task of identifying the authors of South African presidential speeches from State of the Nation Addresses between 1994 and 2023. Using a combination of machine learning techniques, including neural networks, gradient boosting trees, and Electra Small language model, we examine the nuances of political discourse analysis. Our experiments demonstrate the effectiveness of each method. Electra Small, a powerful language model, excels in authorship prediction due to its ability to capture intricate linguistic patterns. However, this advantage comes at the cost of a high parameter count. Our findings indicate that traditional machine learning approaches like neural networks and gradient boosting trees remain vital, especially in resource-constrained contexts. These methods offer competitive accuracy with lower computational demands, making them practical for real-time authorship prediction. This research advances our understanding of speech authorship prediction, emphasizing the need to balance accuracy and resource efficiency in such endeavors."
  },
  {
    "objectID": "writeup.html#abstract",
    "href": "writeup.html#abstract",
    "title": "A Quarto Website",
    "section": "",
    "text": "This study explores the challenging task of identifying the authors of South African presidential speeches from State of the Nation Addresses between 1994 and 2023. Using a combination of machine learning techniques, including neural networks, gradient boosting trees, and Electra Small language model, we examine the nuances of political discourse analysis. Our experiments demonstrate the effectiveness of each method. Electra Small, a powerful language model, excels in authorship prediction due to its ability to capture intricate linguistic patterns. However, this advantage comes at the cost of a high parameter count. Our findings indicate that traditional machine learning approaches like neural networks and gradient boosting trees remain vital, especially in resource-constrained contexts. These methods offer competitive accuracy with lower computational demands, making them practical for real-time authorship prediction. This research advances our understanding of speech authorship prediction, emphasizing the need to balance accuracy and resource efficiency in such endeavors."
  },
  {
    "objectID": "writeup.html#introduction",
    "href": "writeup.html#introduction",
    "title": "A Quarto Website",
    "section": "Introduction",
    "text": "Introduction\nThe study of presidential speeches has been a focal point of political analysis for decades, offering insights into the rhetoric, style, and policy priorities of various administrations. With the recent rise in popularity of machine learning, and specifically Natural Language Processing (NLP), a new approach has emerged: text classification. This paper performs the task of classifying sentences based on the South African presidents who said them. The aim is to compare the performance of large language models (LLMs) with smaller, more efficient models in this classification task.\nThe need for this work arises from the increasing prevalence and importance of LLMs in fields such as NLP and computational linguistics. These models have shown remarkable performance in various tasks, including text generation, translation, question answering, and more (Naveed et al., 2023). However, their development and running costs, as well as their computational requirements, make them less accessible for many organizations (Hart et al., 2023). This creates a gap between what the scientific community currently has - powerful but expensive and resource-intensive models - and what it wants - equally effective models that are more efficient and accessible.\nTo address this need, we have conducted a comparison of LLMs and smaller, fine-tuned models in the task of classifying sentences of South African presidents. We have explored both the theoretical underpinnings of these models and their practical performance in the classification task. Fine-tuned models are generally smaller than their LLM counterparts and are derived from existing language models. They can perform tasks with less data and computational power compared to large language models.\nThe remainder of this paper is structured as follows: we first provide a comprehensive background on large language models and fine-tuned models, discussing their mechanisms, strengths, and weaknesses. We then detail our methodology for comparing these models in the task of classifying presidential sentences. Following this, we present and discuss our results, offering insights into the performance of large and fine-tuned models in this context. Finally, we conclude with a reflection on the implications of our findings for the future development and use of language models in political analysis and beyond."
  },
  {
    "objectID": "writeup.html#literature-review",
    "href": "writeup.html#literature-review",
    "title": "A Quarto Website",
    "section": "Literature review",
    "text": "Literature review\nNatural Language Processing (NLP) has been utilized extensively for diverse tasks. These tasks range from classification in healthcare [(Ong et al., 2010), (Fujita et al., 2012), (Ravindranath et al., 2017), (McKnight, 2012), (Wong & Akiyama, 2013), (Gerdes & Hardahl, 2013)], to predicting financial trends [(Wen et al., 2019), (Wu et al., 2012), (Al-Rubaiee et al., 2015), (Vijayan and Potey, 2016), (Nassirtoussi et al., 2015), (Nikfarjam et al., 2010)], to applications in corporate finance [(Guo et al., 2016), (Shahi et al., 2014), (Holton, 2009), (Chan and Franklin, 2011), (Humpherys et al., 2011), (Loughran and McDonald, 2011)] and the techniques and methodologies employed vary based on the specifics of the problem at hand.\nFor example (Dogra et al., 2022) focusses on exploring cutting-edge learning models implemented on a language dataset. Amongst the models investigated are Naive Bayes models. Through a comprehensive review of this algorithm, the authors provide valuable insights into the nuances of text classification, shedding light the strengths and limitations of the Naive Bayes method. (Dogra et al., 2022) also analyses data formatting techniques such as bag-of-words (BOW), Term Frequency - Inverse Document Frequency (TF-IDF), and word embeddings. BOW is most commonly used in document transformation, however suffers from high dimensionality, resulting in slow compute time. TF-IDF overcomes some weaknesses of BOW, as the text is represented with weighted frequencies, however still suffers from high dimensionality which affects a model’s rate of learning. Word embeddings allow for a reduced dimensional dataset to be analysed by representing each sentence by a vector of fixed size, where each unique word is a unique integer. This allows for sentences to be represented in high dimensional space.\nIn 2019, (Devlin et al., 2019) developed the language model known as BERT which borrowed the Encoder from the previously created Transformer architecture (Vaswani et al., 2017). Although upon its release BERT set new standards for numerous NLP related performance metrics, it does have its shortcomings. Due to the nature of BERT’s training approach being one of masking a fraction of the input tokens and asking the model to predict the token being masked, it doesn’t efficiently utilize all of it’s data, with predictions being made on only 15% of the tokens in each batch (Devlin et al., 2019). (Clark et al., 2020) developed ELECTRA to handle just this problem. Instead of training a model to predict the original identity of a corrupted token, ELECTRA is a discriminator trained to predict whether each token in the corrupted input was replaced by a generator sample or not. This approach to training is shown to be more efficient than the Masked Language Model apporach to training because the task is performed on all input tokens, as oppose to just a select few, and it resulted in ELECTRA achieving a better General Language Understanding Evaluation (GLUE) score than BERT."
  },
  {
    "objectID": "writeup.html#methods",
    "href": "writeup.html#methods",
    "title": "A Quarto Website",
    "section": "Methods",
    "text": "Methods\n\nData preprocessing\nTo get the data into a format compliant with the chosen language models, numerous cleaning operations had to be performed. Firstly, to identify the presidents associated with each speech, a regular expression pattern was defined. This pattern extracted the president’s name from the document names, which are named after the president which delivered the speech contained in the text document. A tabular data structure was then initialized with two columns: ‘Completion’ and ‘Prompt’. This DataFrame serves as the container for organizing the processed data. For each data file, the content is read in while disregarding the initial two lines, which contains the date and an empty line. The content is stored as a list of lines. These lines are then concatenated into a single text document, before being stripped of symbols, such as ‘’. The full text is then tokenized into sentences, with the name of the president inserted into the ‘Presidents’ column next to each sentence. Stop words where then removed too, which reduces the amount of unnessecary data. This resulted in a dataset with 9337 sentences, and served as a starting point for 3 other datasets to be formed.\n\nBag of Words (BoW)\nBoW (Harris, 1954) is a data manipulating technique which has a variety of use cases, such as in Image Classification, Text Classification and Visual Scene Classification (Qader et al., 2019). In the context of visual scene classification, the BoW approach allows for clusters of local descriptors to be extracted from the images, where the order of the clusters is not important. In the domain of text classification, which is the field of this study, the BoW methodology entails counting the number of occurrences associated with each word within a given sentence, while disregarding the inherent word order and grammatical structure. The fundamental concept underlying BoW is the generation of a histogram, capturing the frequency distribution of words within textual documents or the prevalence of features within images, thereby creating a representative profile for the respective sentence or image which attempt to capture the key aspects. Notably, the BoW technique is characterized by its computational simplicity, rendering it more accessible and conceptually straightforward than many alternative classification methodologies (Qader et al., 2019). Consequently, BoW-based systems have demonstrated the potential to achieve improved performance metrics on widely accepted benchmarks for evaluating text and image classification algorithms.\nIn order to transform our dataframe consisting of a ‘Completion’ and ‘Prompt’ column into a BoW dataset, 3 steps need to be followed. Firstly, Text Tokenization needs to occur, which involves the segmentation of input text into individual words or terms, commonly referred to as tokens. Then a vocabulary has to be formed which is a dictionary comprising of all the distinct words (tokens) identified within the text corpus. Each of these unique words is associated with a specific integer index. Finally, Token Frequency Counting must occur. At this stage, the number of occurrences of each word (token) within every sentence present in the text corpus must be counted. The outcome is a matrix where each row corresponds to a sentence, and each column corresponds to a unique word from the vocabulary. The entries in this matrix reflect the frequency (count) of each word within each sentence. Thus, if the matrix is called \\(D\\), entry \\(D_{i,j}\\) is the number of times word \\(j\\) appeared in sentence \\(i\\). These 3 steps are easily performed using sklearn (Buitinck et al., 2013) and more specifically the CountVectorizer() and fit_transform() functions.\n\n\nTerm Frequency - Inverse Document Frequency (TF-IDF)\nTF-IDF (Luhn, 1957) is a numerical statistic used in text mining and information retrieval in order to reflect how important a word is in a document in a collection or corpus based on two factors. The TF-IDF value increases proportionally to the number of times a word appears in the document and is decreases based on the number of documents in the corpus that contain the word. This helps to offset the frequency bias which would declare commonly used words as more important than smaller ones. TF-IDF is thus composed of two terms, Term Frequency (TF) and Inverse Document Frequency (IDF). TF measures the frequency of a word in a document, thus if a word appears more often in a document, its TF value increases according to the following formula:\n\\[\n\\text{TF(t)} = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}\n\\]\nIDF measures the importance of a word across a set of sentences, in our case, but can be documents in other cases. If a word appears in many sentences, it’s not a unique identifier of that sentence, therefore its IDF value decreases. The IDF of a word \\(t\\) can be calculated as follows:\n\\[\n\\text{IDF(t)} = ln \\left( \\frac{\\text{Total number of sentences}}{\\text{Number of sentences with term t in it}}\\right )\n\\]\nThe TF-IDF value of a word in a particular sentence is the product of its TF and IDF values, thus: \\[\n\\text{TF-IDF(t)} = \\text{TF(t)} \\times \\text{IDF(t)}\n\\]\nThis value is thus larger for words that are more unique to a sentence, and lower for words that are common across many sentences.\nA key difference between the BoW model and TF-IDF is that BoW can be seen to suffer from frequency bias as it does not consider the uniqueness of words to a sentence, but rather places importance based on frequency. Thus common words in our case like ‘Government’ and ‘Country’ will be valued highly simply because they are used often. However, in the TF-IDF approach these words will have low scores as they appear in many documents, and therefore are not unique or informative.\nIn order to convert our dataset of a ‘Completion’ and ‘Prompt’ column into a TF-IDF dataset, similar steps shown above must be followed. Again, tokenization must first occur, where input text is decomposed into discrete words (tokens). This process partitions the text into individual words or terms, making it amenable to further analysis. Following tokenization, a dictionary containing all distinct words (tokens) encountered within the text corpus must be built. Each unique token within this vocabulary is assigned a unique integer index for identification. Then TF-IDF Transformation occurs. This diverges from the conventional token counting approach, as the TF, IDF, and then TF-IDF values for each token within every sentence is calculated. The outcome of this process is a TF-IDF matrix. Similarly to the BoW matrix, each row within the matrix corresponds to an individual sentence, while each column corresponds to a unique token derived from the vocabulary. The matrix entries however signify the computed TF-IDF values for each token within each document, encapsulating the importance and discriminatory potential of each term. Once again, this process is largely simplified by utilizing the sklearn package and more specifically the TfidfVectorizer() and fit_transform() functions.\n\n\nWord Embeddings\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed, numeric representation of text that is perhaps one of the key reasons for the impressive performance of deep learning methods on challenging NLP problems (Mikolov et al., 2013). State of the art Word embeddings map words in a vocabulary into a vector space, where the position of each word is learned from text data and is based on the context in which it is used. Thus typically, the position of a word within the learned vector space is referred to as its embedding. Word embeddings differ from BoW or TF-IDF datasets in several key ways. Firstly, word embeddings can capture much more complex constructs, like the semantic meaning of words. Words are thus defined by the surrounding words, which encourages the model to learn context. BoW and TF-IDF on the other hand disregard word order and context. Word embeddings can also have much lower dimensionality, which can reduce the compute time far greater than is the case with BoW and TF-IDF.\nHowever, for this study, a simple frequency based word embedding system is used which gives values to words based soley on the frequency of their occurence. In order to transform our dataset of a ‘Completion’ and ‘Prompt’ column into a word embedded dataset, 4 steps need to be followed. Firstly, an upper bound of 1000 unique words is set to restrict the number of words used in the embedding. This is done to increase efficiency. Each unique word in the text corpus needs to then be ranked in descending order of frequency, thus with the most frequently used word given a value of 1, and the second most frequently used word given a value of 2, etc. Then each sentence in the dataset should be iterated through, converting each word to the corresponding number in the frequency based ranking, while removing words entirely if they do not appear within the top 1000 most frequently used words. Finally, to ensure each sentence is embedded as a vector of the equal length, an upper bound of 50 was chosen such that vectors of length less than 50 were padded with zeros, and vectors of length over 50 were truncated to the first 50 integers.\n\n\nBERT Embeddings\nThe BERT Embedding model (Devlin et al., 2019) is a pretrained model which takes one or two sentences and embeds it in a vector of numbers, similar to what we saw above. BERT’s state-of-the-art performance is based on new pre-training tasks, namely Masked Language Model(MLM) and Next Sentense Prediction (NSP), as well as a large enough dataset(3.3 billion words) and enough compute power to train BERT. Through MLM and NSP, BERT is able to attach meaning to words through recognizing the word itself, as well as by analyzing the words around it.\n\n\n\nEDA\nThe dataset is comprised of 36 speeches made by South African presidents Zuma, Mbeki, Ramaphosa, Mandela, Motlanthe, and de Klerk, from the years 1994 to 2023. The barplot below displays the number of sentences spoken by each president. Significant variation in the number of sentences associated with each president can be seen. Notably, Jacob Zuma had the highest number of sentences attributed to him, totaling 2,629 sentences. Thabo Mbeki followed closely with 2,397 sentences, while Cyril Ramaphosa had 2,279 sentences. Nelson Mandela had 1,671 sentences in the dataset. Kgalema Motlanthe and F.W. de Klerk had relatively fewer sentences, with 264 and 97 sentences, respectively.\n\n\n\n\n\n\n\n\n\n\nFindings include varying sentence lengths among presidents, with President Mbeki having the longest average sentence length at approximately 15.27 words per sentence, and President de Klerk having the shortest at about 9.87 words per sentence. This variability may indicate differences in communication styles, speech contexts, or historical periods.\nThe word clouds below shows the most frequently used words by each president. Interestingly, de Klerk is the only president who regularly used the words “constitution” and “constitutional”, which could be due to him being involved in the negotiation of the new constitution in South Africa. Other commonly used words which further suggest this narrative include “right”, “amendment” and “Freedom Alliance”. “South Africa”, “Goverment” and “country” are some of the other more popular words used by the presidents, as can be seen in Mandela’s wordcloud. Interestingly, before removing the stopwords, “will” was a very popular word used by the presidents which highlights the alure of promises about the future often made by leaders.\n\n\n\n\n\n\n\n\n\n\nWith the comprehensive description and analysis of the data completed, we can now delve into the intricate details of the models developed for this classification task.\n\n\nModelling\n\nFeed-forward Neural Networks\nFeed-forward Neural networks are a subset of machine learning algorithms modeled after the human brain (Lim & Sung, 1970). They are composed of interconnected nodes or “neurons” that process information using a system of weights and bias values. These weights and bias values are adjusted during the training process to minimize the difference between the network’s predicted output and the actual observed value. This updating of weights occurs threw a process known as backpropagation. The advantage of neural networks is their ability to identify complex patterns and relationships in data, making them especially useful for tasks such as image recognition, natural language processing, and predictive modeling.\nTo optimize the performance of the Feed-forward Neural Network, a systematic exploration was undertaken. This involved conducting a grid search, wherein various configurations were evaluated. Specifically, we focused on 3 critical aspects of the network architecture: the number of hidden layers, the number of neurons within each layer, and the L2-regularization value applied. Our initial exploration encompassed three distinct network architectures. First, a model with two hidden layers was examined, followed by one with three hidden layers and finally a model with four hidden layers. The selection of the ultimate network architecture was based on the examination of the validation accuracy. Having determined the optimal architecture, we further refined our model through the application of L2-regularization. In particular, we applied a range of L2-regularization values from 0.01 to 0.1, with the intent of identifying the value that yielded the highest validation accuracy in the selected model. This approach was replicated for each of the datasets, namely BoW, TF-IDF, and word embeddings.\n\n\nBoosted Trees\nBoosted trees are an ensemble of, generally, weak learners, which are sequentially trained on the errors made by their predecessors. This iterative process involves assigning weights to training data points, emphasizing the samples that are consistently misclassified. The subsequent trees are then tailored to rectify these misclassifications, incrementally improving predictive accuracy over a large number of learners. CatBoost, as introduced by (Prokhorenkova et al., 2019), distinguishes itself from the other gradient boosting algorithms such as XGBoost and LightGBM through its unique approach to tree construction. Unlike its counterparts, CatBoost constructs balanced trees that exhibit a symmetrical structure. This implies that the feature-split pair which minimizes the loss function is uniformly selected and applied to all nodes within the same level of the tree.\nIn the pursuit of optimizing the hyperparameters of the CatBoost algorithm, a grid search was conducted. The grid search was performed across a comprehensive parameter spacethe parameters iterations, learning_rate, depth, and l2_leaf_reg. iterations determines the number of boosting iterations during the training process. learning_rate governs the step size at each iteration while moving towards a minimum of the loss function, while depth defines the maximum depth of the decision tree. Finally, l2_leaf_reg contributes to controlling overfitting by penalizing the magnitude of the weights. For each combination of these parameters, the CatBoost algorithm was trained and validated, and the model’s performance was assessed through the accuracy of the model on the validation set. The overarching objective was to identify the hyperparameter configuration that yielded the optimal model performance, as indicated by the validation accuracy.\n\n\nNaive Bayes\nThe Naive Bayes algorithm is a classification technique based on Bayes’ Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event (Dogra et al., 2022). The algorithm is labeled “naive” because it assumes that all features in a dataset are mutually independent. Despite this oversimplification, Naive Bayes can be remarkably accurate, fast, and efficient, particularly with large datasets and it is widely used in text classification.\nIn order to optimise the Naive Bayes algorithm, the single alpha parameter was chosen such that the validation accuracy was once again maximised. alpha is instrumental in preventing zero proabilities occuring with words that are not found in all datasets, thus alpha being set to zero can greatly reduce the performance of the model. alpha is generally set to 1, also known as Laplace smoothing, however optimal values depend on the dataset.\n\n\nELECTRA\nThe ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)(Clark et al., 2020) model introduces a novel approach to pre-training in the domain of natural language processing (NLP). While traditional pre-training methods like BERT rely on Masked Language Modeling (MLM), where input tokens are partially obscured by replacing them with a special token ([MASK]) and the model is trained to predict the original tokens, ELECTRA takes a distinct and more data-efficient route. ELECTRA replaces select tokens in the input text with plausible alternatives drawn from a generator network. The important change is in the subsequent training objective: rather than training a model to predict the original identities of the corrupted tokens, ELECTRA trains a discriminative model, like a Generative Adverserial Network (GAN) (Goodfellow et al., 2014). The Discriminative model’s task is thus to determine whether each token in the corrupted input was replaced by a sample generated from the network or if it is the original, unchanged token.\n\n\n\nModel diagnostics\nNumerous model evaluation scores were consulted during the modelling process which allowed for the relative and absolute competence of the models to be seen. Amongst the metrics investigated, is the training accuracy, which shows the ability of the model to learn from the training set, however cannot be taken as absolute model performance. For this, the out-of-sample, test set accuracy, which measures the proportion of sentences correctly attriuted to a president, must be consulted. This provides a stong indicication for the models performance as it is performed on unseen data. Precision, which measures the proportion of true positive predictions for each class among all positive predictions for that class was also consulted. This allows for a more indepth understanding of how the model performs on each class.\n\\[\n\\text{Precision}= \\frac{\\text{True Positives for Class}}{\\text{True Positives for Class} + \\text{False Positives for Class}}\n\\]\nAnother class specific metric is recall, which measures the proportion of true positive predictions for each class among all actual instances of that class.\n\\[\n\\text{Recall}= \\frac{\\text{True Positives for Class}}{\\text{True Positives for Class} + \\text{False Negatives for Class}}\n\\]\nThe final metric consulted is the F1-score for each class and is the harmonic mean of precision and recall for that class. It balances precision and recall and since our classes are imbalanced, it is a suitable metric for model performance (Seo et al., 2021).\n\\[\n\\text{F1-Score}= 2\\times \\frac{\\text{Preicision}\\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]"
  },
  {
    "objectID": "writeup.html#results",
    "href": "writeup.html#results",
    "title": "A Quarto Website",
    "section": "Results",
    "text": "Results\nThe table below illustrates the test set accuracy for various machine learning models across the different datasets. Notably, we observe variations in accuracy across models and data sources, which highlight the influence of both the model architecture and the data representation technique. The highest test set accuracy is achieved with the Neural Network on the TF-IDF, with a score of 59.7%, while the Naive Bayes model performs second best also on the TF-IDF dataset with a score of 59%. The Boosted Tree performs best on the Frequency embeddings with a score of 34.8%.\n\n\n\n\n\n\n\n\n\n\nModel\nBoW\nTF-IDF\nFrequency Embeddings\nBERT Embeddings\n\n\n\n\nNeural Net\n0.567\n0.597\n0.343\n\n\n\nBoosted Tree\n0.549\n0.555\n0.348\n\n\n\nNaive Bayes\n0.580\n0.590\n0.300\n\n\n\nELECTRA-Base\n\n\n\n0.580\n\n\n\nTalk about F1-scores below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Type\nDataset\nP1\nP2\nP3\nP4\nP5\nP6\n\n\n\n\nNeural Net\nBoW\n0.098\n0.442\n0.551\n0.606\n0.612\n0.615\n\n\n\nTF-IDF\n0.045\n0.481\n0.572\n0.647\n0.615\n0.583\n\n\n\nEmbeddings\n0.0\n0.042\n0.219\n0.112\n0.432\n0.0\n\n\nCatboost\nBoW\n0.044\n0.465\n0.532\n0.583\n0.612\n0.381\n\n\n\nTF-IDF\n0.047\n0.493\n0.545\n0.581\n0.612\n0.3\n\n\n\nEmbeddings\n0.0\n0.008\n0.39\n0.235\n0.453\n0.0\n\n\nNaive Bayes\nBoW\n0.06\n0.46\n0.58\n0.65\n0.62\n0.08\n\n\n\nTF-IDF\n0.04\n0.5\n0.6\n0.65\n0.65\n0.0\n\n\n\nEmbeddings\n0.02\n0.11\n0.31\n0.18\n0.47\n0.01\n\n\n\nDiscussion Section: The accuracy table provides valuable insights into the performance of different machine learning models across multiple data sources. Noteworthy observations include the relatively high accuracy achieved by the “Neural Net” and “Boosted Tree” models when applied to TF-IDF data, underscoring the suitability of this data representation for these models. In contrast, the “Naive Bayes” model excels with BoW data. Additionally, the introduction of BERT Embeddings, represented by the “ELECTRA-Base” model, demonstrates significantly enhanced accuracy, suggesting the effectiveness of pre-trained embeddings in capturing complex patterns within the data. These findings emphasize the importance of model-data compatibility and the potential benefits of leveraging advanced embeddings for improved predictive performance. The table offers a comprehensive view of the interplay between models and data sources, shedding light on their combined influence on the accuracy of the predictive models."
  },
  {
    "objectID": "writeup.html#discussion",
    "href": "writeup.html#discussion",
    "title": "A Quarto Website",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "writeup.html#bibliography",
    "href": "writeup.html#bibliography",
    "title": "A Quarto Website",
    "section": "Bibliography",
    "text": "Bibliography\nLuo, X. and AbstractText classification (TC) is an approach used for the classification of any kind of documents for the target category or out. In this paper (2021) Efficient English text classification using selected Machine Learning Techniques, Alexandria Engineering Journal. Available at: https://www.sciencedirect.com/science/article/pii/S1110016821000806 (Accessed: 05 October 2023).\nDogra, V. et al. (2022) A complete process of text classification system using state-of-the-art NLP models, Computational intelligence and neuroscience. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9203176/ (Accessed: 05 October 2023).\nLim, J. and Sung, K. (1970) FNN (Feedforward Neural Network) training method based on robust recursive least square method, SpringerLink. Available at: https://link.springer.com/chapter/10.1007/978-3-540-72393-6_48 (Accessed: 05 October 2023).\nOng, M.-S., Magrabi, F. and Coiera, E. (2010) Automated categorisation of Clinical Incident Reports Using Statistical Text Classification, BMJ Quality & Safety. Available at: https://qualitysafety.bmj.com/content/19/6/e55 (Accessed: 07 October 2023).\nRavindranath, P. et al. (2017) Machine learning in Automated Classification of adverse events in clinical studies of alzheimer’s disease, Alzheimer’s & Dementia. Available at: https://www.sciencedirect.com/science/article/pii/S1552526017321064 (Accessed: 07 October 2023).\nMcKnight. (2012) Semi-supervised classification of Patient Safety Event Reports, Journal of patient safety. Available at: https://pubmed.ncbi.nlm.nih.gov/22543364/ (Accessed: 07 October 2023).\nWong, Z. and Akiyama, M. (2013) Statistical text classifier to detect specific type of medical incidents, Studies in health technology and informatics. Available at: https://pubmed.ncbi.nlm.nih.gov/23920827/ (Accessed: 07 October 2023).\nGerdes, L. and Hardahl, C. (2013) Text mining electronic health records to identify hospital adverse events, Studies in health technology and informatics. Available at: https://pubmed.ncbi.nlm.nih.gov/23920919/ (Accessed: 07 October 2023).\nWen F, Xu L, Ouyang G, Kou G. (2019). Retail investor attention and stock price crash risk: evidence from China. Int Rev Financ Anal 65:101376. Available at: https://doi.org/10.1016/j.irfa.2019.101376 (Accessed: 07 October 2023).\nWu JL, Su CC, Yu LC, Chang PC (2012). Stock price predication using combinational features from sentimental analysis of stock news and technical analysis of trading information. Int Proc Econ Dev Res. Available at: https://doi.org/10.7763/ipedr (Accessed: 07 October 2023).\nAL-Rubaiee H, Qiu R, Li D (2015). Analysis of the relationship between Saudi twitter posts and the Saudi stock market. In: 2015 IEEE seventh international conference on intelligent computing and information systems (ICICIS). Available at: https://doi.org/10.1109/intelcis.2015.7397193 (Accessed: 07 October 2023).\nVijayan R, Potey MA (2016). Improved accuracy of FOREX intraday trend prediction through text mining of news headlines using J48. Int J Adv Res Comput Eng Technol 5(6):1862–1866. Available at: https://www.semanticscholar.org/paper/Improved-Accuracy-of-FOREX-Intraday-Trend-through-J-Vijayan-Potey/7f920ff986d2845fe5eb1ef1a4372951afbb21a4 (Accessed: 07 October 2023).\nNassirtoussi AK, Aghabozorgi S, Wah TY, Ngo DC (2015). Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment. Expert Syst Appl 42(1):306–324. https://doi.org/10.1016/j.eswa.2014.08.004 (Accessed: 07 October 2023).\nNikfarjam A, Emadzadeh E, Muthaiyah S (2010). Text mining approaches for stock market prediction. In: 2010 the 2nd international conference on computer and automation engineering (ICCAE). Available at: https://doi.org/10.1109/iccae.2010.5451705 (Accessed: 07 October 2023).\nGuo L, Shi F, Tu J (2016). Textual analysis and machine leaning: crack unstructured data in finance and accounting. J Finance Data Sci 2(3):153–170. Available at: https://www.sciencedirect.com/science/article/pii/S2405918816300496 (Accessed: 07 October 2023).\nShahi AM, Issac B, Modapothala JR (2014). Automatic analysis of corporate sustainability reports and intelligent SCORING. Int J Comput Intell Appl 13(01):1450006. Available at: https://doi.org/10.1142/s1469026814500060 (Accessed: 07 October 2023).\nHolton C (2009). Identifying disgruntled employee systems fraud risk through text mining: a simple solution for a multi-billion dollar problem. Decis Support Syst 46(4):853–864. Available at: https://www.sciencedirect.com/science/article/pii/S0167923608002078 (Accessed: 07 October 2023).\nChan SWK, Franklin J (2011). A text-based decision support system for financial sequence prediction. Decis Support Syst 52(1):189–198. Available at: https://www.sciencedirect.com/science/article/pii/S0167923611001230 (Accessed: 07 October 2023).\nHumpherys SL, Moffitt KC, Burns MB, Burgoon JK, Felix WF (2011). Identification of fraudulent financial statements using linguistic credibility analysis. Decis Support Syst 50(3):585–594. Available at: https://www.sciencedirect.com/science/article/pii/S0167923610001338 (Accessed: 07 October 2023).\nLoughran T, Mcdonald B (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. J Finance 66(1):35–65. Available at: https://www.sciencedirect.com/science/article/pii/S0167923610001338 (Accessed: 07 October 2023).\nNaveed, H. et al. (2023). A comprehensive overview of large language models, arXiv.org. Available at: https://arxiv.org/abs/2307.06435 (Accessed: 08 October 2023).\nHart, S. et al. (2023) Organizational preparedness for the use of large language models in pathology informatics, Journal of Pathology Informatics. Available at: https://www.sciencedirect.com/science/article/pii/S2153353923001529 (Accessed: 08 October 2023).\nVaswani, A. et al. (2017) Attention is all you need, arXiv.org. Available at: https://arxiv.org/abs/1706.03762 (Accessed: 08 October 2023).\nClark, K. et al. (2020) Electra: Pre-training text encoders as discriminators rather than generators, arXiv.org. Available at: https://arxiv.org/abs/2003.10555 (Accessed: 08 October 2023).\nDevlin, J. et al. (2019) Bert: Pre-training of deep bidirectional Transformers for language understanding, arXiv.org. Available at: https://arxiv.org/abs/1810.04805 (Accessed: 08 October 2023).\nSeo, S. et al. (2021) Predicting successes and failures of clinical trials with outer product-based convolutional neural network, Frontiers in pharmacology. Available at: https://pubmed.ncbi.nlm.nih.gov/34220508/ (Accessed: 08 October 2023).\nHarris, Z.S. (1954) Distributional structure zellig S. harris - taylor & francis online. Available at: https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520 (Accessed: 09 October 2023).\nQader, W.A., Ameen, M.M. and Ahmed, B.I. (2019) An overview of bag of words;importance, implementation, applications, and Challenges. Available at: https://www.researchgate.net/publication/338511771_An_Overview_of_Bag_of_WordsImportance_Implementation_Applications_and_Challenges (Accessed: 09 October 2023).\nBuitinck, L. et al., 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning. pp. 108–122.\nLuhn, H.P. (1957) A Statistical Approach to Mechanized Encoding and Searching of Literary Information, IEEE Xplore. Available at: https://ieeexplore.ieee.org/document/5392697 (Accessed: 09 October 2023).\nMikolov, T. et al. (2013) Efficient estimation of word representations in vector space, arXiv.org. Available at: https://arxiv.org/abs/1301.3781 (Accessed: 09 October 2023).\nGoodfellow, I.J. et al. (2014) Generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1406.2661 (Accessed: 11 October 2023)."
  },
  {
    "objectID": "writeup.html#figures",
    "href": "writeup.html#figures",
    "title": "A Quarto Website",
    "section": "Figures",
    "text": "Figures\n\n\n\nFigure 1"
  },
  {
    "objectID": "allcode.html",
    "href": "allcode.html",
    "title": "Welcome to all my code",
    "section": "",
    "text": "In this section, we will perform data cleaning and prepare various datasets for analysis.\n\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nExtract names of all data files, store in files array, and extract names of all presidents and store in president_names array.\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n\n\n\n\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  data = pd.read_csv(\"data.csv\")\n  y = data['completion']\n  return(y)\n\n\n\n\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n    return tfidf_df"
  },
  {
    "objectID": "allcode.html#data-cleaning",
    "href": "allcode.html#data-cleaning",
    "title": "Welcome to all my code",
    "section": "",
    "text": "In this section, we will perform data cleaning and prepare various datasets for analysis.\n\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nExtract names of all data files, store in files array, and extract names of all presidents and store in president_names array.\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n\n\n\n\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  data = pd.read_csv(\"data.csv\")\n  y = data['completion']\n  return(y)\n\n\n\n\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n    return tfidf_df"
  },
  {
    "objectID": "allcode.html#word-embedding",
    "href": "allcode.html#word-embedding",
    "title": "Welcome to all my code",
    "section": "Word Embedding",
    "text": "Word Embedding\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef embeddings_data_prep():\n    data = pd.read_csv(\"data.csv\").iloc[:, 1:]\n\n    # Tokenization\n    max_features = 1000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data['prompt'])\n    sequences = tokenizer.texts_to_sequences(data['prompt'])\n\n    # Filter out empty sequences and corresponding labels\n    filtered_indices = [i for i, s in enumerate(sequences) if len(s) &gt; 0]\n    sequences = [sequences[i] for i in filtered_indices]\n    y = data['completion'].iloc[filtered_indices].values\n\n    # Splitting data into training, validation, and test sets\n    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)\n    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n    maxlen = 50\n    x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n    x_val_pad = pad_sequences(x_val, maxlen=maxlen)\n    x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n\n    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test\n\n\n#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()"
  },
  {
    "objectID": "allcode.html#exploratory-data-analysis",
    "href": "allcode.html#exploratory-data-analysis",
    "title": "Welcome to all my code",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nNumber of sentences per president.\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndata = pd.read_csv(\"data.csv\").iloc[:,1:]\nsentence_counts = data['completion'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=sentence_counts.index, y=sentence_counts.values)\n\nplt.xlabel('Presidents')\nplt.ylabel('Number of Sentences')\nplt.savefig('eda_plots/sentence_counts.png', bbox_inches='tight')\nplt.show()\n\nSentence length plot.\n\ndata['sentence_length'] = data['prompt'].apply(lambda x: len(x.split()))\naverage_sentence_length = data.groupby('completion')['sentence_length'].mean().reset_index()\ndesired_order = [4, 2, 3, 1, 0, 5]\naverage_sentence_length = average_sentence_length.loc[desired_order]\n# Plot the barplot of average sentence lengths per president\nplt.figure(figsize=(10, 6))\nsns.barplot(x='completion', y='sentence_length', data=average_sentence_length)\nplt.xlabel('Presidents')\nplt.ylabel('Average Sentence Length')\nplt.savefig('eda_plots/sentence_length.png', bbox_inches='tight')\nplt.show()\n\nProduce word cloud.\n\npresidents = data['completion'].unique()\nfor president in presidents:\n    text = \" \".join(data[data['completion'] == president]['prompt'])\n    \n    # Create a WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='grey').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    #plt.title(f'Word Cloud for President {president}')\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "allcode.html#models",
    "href": "allcode.html#models",
    "title": "Welcome to all my code",
    "section": "Models",
    "text": "Models\n\nNeural Network\n\n# Import required libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport sklearn\nimport pickle\n#from bow import bow_x, bow_y\n\n# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Keras specific\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nx = bow_x()\ny = bow_y()\ndef data_prep(x,y):\n    X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42)\n    # Split your data into training and validation sets\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    X_train = X_train.values\n    X_val = X_val.values\n    X_test = X_test.values\n\n    label_encoder = LabelEncoder()\n\n    # Fit and transform the labels to integer labels\n    y_train_encoded = label_encoder.fit_transform(y_train)\n    y_val_encoded = label_encoder.fit_transform(y_val)\n    y_test_encoded = label_encoder.fit_transform(y_test)\n\n    # Convert to one-hot encoded vector\n    y_train = to_categorical(y_train_encoded)\n    y_val = to_categorical(y_val_encoded)\n    y_test = to_categorical(y_test_encoded)\n\n    # dimensions\n    inp_dim = X_test.shape[1]\n    count_classes = y_test.shape[1]\n    return {\n        'X_train': X_train,\n        'X_val': X_val,\n        'X_test': X_test,\n        'y_train': y_train,\n        'y_val': y_val,\n        'y_test': y_test,\n        'inp_dim': inp_dim,\n        'count_classes': count_classes\n    }\ndata = data_prep(x,y)\nX_train = data['X_train']\nX_val = data['X_val']\nX_test = data['X_test']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test= data['y_test']\ninp_dim = data['inp_dim']\ncount_classes = data['count_classes']\nnum_epochs = 20\n\n# function to train and test model with specific params\ndef create_custom_model(neurons_per_layer, l2_reg_value):\n    # Create a Sequential model\n    model = Sequential()\n    neurons_per_layer = [500, 200]\n    # Add the input layer with L2 regularization\n    model.add(Dense(neurons_per_layer[0], activation='relu', input_dim=inp_dim, kernel_regularizer=l2(l2_reg_value)))\n\n    # Add hidden layers with L2 regularization\n    for num_neurons in neurons_per_layer[1:]:\n        model.add(Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_reg_value)))\n\n    # Add the output layer with L2 regularization\n    model.add(Dense(count_classes, activation='softmax', kernel_regularizer=l2(l2_reg_value)))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model with validation data\n    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n\n    # Evaluate the model on training and test data\n    scores_train = model.evaluate(X_train, y_train, verbose=1)\n    scores_test = model.evaluate(X_test, y_test, verbose=0)\n    \n\n    # print('Accuracy on training data: {:.2f}%\\nError on training data: {:.2f}'.format(scores_train[1] * 100, (1 - scores_train[1]) * 100))\n    # print('Accuracy on test data: {:.2f}%\\nError on test data: {:.2f}'.format(scores_test[1] * 100, (1 - scores_test[1]) * 100))\n\n    # Access validation scores from the history object\n    val_loss = history.history['val_loss']\n    val_accuracy = history.history['val_accuracy']\n\n    y_pred = model.predict(X_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels\n\n    # Convert one-hot encoded ground truth labels to class labels\n    y_true_classes = np.argmax(y_test, axis=1)\n\n    # Calculate accuracy, precision, recall, and F1 score\n    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n    precision = precision_score(y_true_classes, y_pred_classes, average=None)\n    recall = recall_score(y_true_classes, y_pred_classes, average=None)\n    f1 = f1_score(y_true_classes, y_pred_classes,\n     average=None)\n\n    # print('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n    # print('Precision on test data: {:.2f}'.format(precision))\n    # print('Recall on test data: {:.2f}'.format(recall))\n    # print('F1 score on test data: {:.2f}'.format(f1))\n\n    return {\n        'val_loss': val_loss,\n        'val_accuracy': val_accuracy,\n        'train_loss': history.history['loss'],\n        'train_accuracy': history.history['accuracy'],\n        'test_loss': scores_test[0],\n        'test_accuracy': scores_test[1],\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n        }\n\nCreate 3 model architectures, and implement L2 regularization on best performing one. First with BoW data.\n\nbow_results_2 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\nbow_results_3 = create_custom_model(neurons_per_layer=[500,200,100], l2_reg_value=0)\nbow_results_4 = create_custom_model(neurons_per_layer=[800,400,100,50], l2_reg_value=0)\n\n# bow_results_2[\"test_accuracy\"] #0.5952890515327454\n# bow_results_3[\"test_accuracy\"] #0.5852962136268616\n# bow_results_4[\"test_accuracy\"] #0.6038544178009033\n\nNow regularize the 2 layer Neural Network, since the benefit is incremental, but much less compute needed.\n\nbow_results_2_001 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.01)\n\nbow_results_2_005 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.05)\n\nbow_results_2_01 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.1)\n\nbow_results_2_001[\"test_accuracy\"] #0.5952890515327454\nbow_results_2_005[\"test_accuracy\"] #0.5852962136268616\nbow_results_2_01[\"test_accuracy\"] #0.6038544178009033\n\nbow_results_2_0 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\n\n\nfile_path = 'nn_res/bow_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(bow_results_2_0, file)\n\nNow do the same process but for TF-IDF dataset.\n\n#from tf_idf import idf\n# import idf data\nx = idf()\ndata = data_prep(x,y)\nX_train = data['X_train']\nX_val = data['X_val']\nX_test = data['X_test']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test= data['y_test']\ninp_dim = data['inp_dim']\ncount_classes = data['count_classes']\n\nidf_results_2 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\nidf_results_3 = create_custom_model(neurons_per_layer=[500,200,100], l2_reg_value=0)\nidf_results_4 = create_custom_model(neurons_per_layer=[800,400,100,50], l2_reg_value=0)\n\nidf_results_2[\"test_accuracy\"] #0.600285530090332\nidf_results_3[\"test_accuracy\"] #0.599571704864502\nidf_results_4[\"test_accuracy\"] #0.5888651013374329\n\n\nidf_results_2_001 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.01)\n\nidf_results_2_005 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.05)\n\nidf_results_2_01 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.1)\n\n# no reg is the best\nidf_results_2_0 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\n\n\n# idf_results_2_001[\"test_accuracy\"] #0.600285530090332\n# idf_results_2_005[\"test_accuracy\"] #0.599571704864502\n# idf_results_2_01[\"test_accuracy\"] #0.5888651013374329\nidf_results_2_0[\"test_accuracy\"]\n\nfile_path = 'nn_res/idf_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(idf_results_2_0, file)\n\nAnd train networks for word embeddings.\n\n#from embedding import embeddings_data_prep\n\nX_train, X_val, X_test, y_train, y_val, y_test = embeddings_data_prep()\n# encode the y values\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the labels to integer labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.fit_transform(y_val)\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Convert to one-hot encoded vector\ny_train = to_categorical(y_train_encoded)\ny_val = to_categorical(y_val_encoded)\ny_test = to_categorical(y_test_encoded)\n\n# dimensions\ninp_dim = X_test.shape[1]\ncount_classes = y_test.shape[1]\n\n\n# train models\nnum_epochs = 100\nembeds_results_2 = create_custom_model(neurons_per_layer=[40,10], l2_reg_value=0)\nembeds_results_3 = create_custom_model(neurons_per_layer=[40,30,10], l2_reg_value=0)\nembeds_results_4 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0)\n\nembeds_results_2[\"test_accuracy\"] #0.2942446172237396\nembeds_results_3[\"test_accuracy\"] #0.2942446172237396\nembeds_results_4[\"test_accuracy\"] #0.3179856240749359\n\n# now with reg\n\nembeds_results_2_001 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.01)\n\nembeds_results_2_005 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.05)\n\nembeds_results_2_01 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.1)\n\n\nembeds_results_2_001[\"test_accuracy\"] #0.600285530090332\nembeds_results_2_005[\"test_accuracy\"] #0.599571704864502\nembeds_results_2_01[\"test_accuracy\"] #0.5888651013374329\nnum_epochs = 200\nembeds_results_2_0 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0)\n\n\nfile_path = 'nn_res/embed_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(embeds_results_2_0, file)\n\nPLOT MODEL INFORMATION.\n\nwith open(\"nn_res/bow_results_2_0.pkl\", 'rb') as file:\n    bow_res = pickle.load(file)\n\nwith open(\"nn_res/idf_results_2_0.pkl\", 'rb') as file:\n    idf_res = pickle.load(file)\n\nwith open(\"nn_res/embed_results_2_0.pkl\", 'rb') as file:\n    embed_res = pickle.load(file)\n\nbow_val = bow_res[\"val_accuracy\"]\nidf_val = idf_res[\"val_accuracy\"]\nembed_val = embed_res[\"val_accuracy\"][::10]\nx_bow = range(len(bow_val))\nx_idf = range(len(idf_val))\nx_embed = range(len(embed_val))\nx_ticks = list(range(20))\n\nfig, ax = plt.subplots()\nax.plot(x_bow, bow_val, label='BoW')#, marker='o')\nax.plot(x_idf, idf_val, label='IDF')#, marker='s')\nax.plot(x_embed, embed_val, label='Embeddings')#, marker='^')\n\n# Set labels and title\nax.set_xlabel('Epochs')\nax.set_ylabel('Validation Accuracy')\nax.set_title('Validation Accuracy Comparison')\nax.legend()\nax.set_xticks(range(21))\nplt.grid(True)\nplt.savefig('nn_res/nn_val_acc.png', bbox_inches='tight')\nplt.show()\n\n\n\nBoosted Tree\n\nimport catboost as cb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport re\nimport pickle\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom catboost import CatBoostClassifier\nimport collections\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n# from bow import bow_x, bow_y\n# from tf_idf import idf\n# from embedding import embeddings_data_prep\n\nFirst we do catboost on BoW data.\n\n# SPLIT INTO BAG OF WORDS FORMAT\nx = bow_x()\ny = bow_y()\n# split data train,val,test\n# Split the data into training (70%), validation (15%), and test (15%) sets\nx_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42)\n\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n# BRING DATA: SPLIT INTO TRAIN, VALIDATION AND TEST\ntrain_dataset = cb.Pool(x_train, y_train) \nval_dataset = cb.Pool(x_val, y_val) \ntest_dataset = cb.Pool(x_test, y_test)\n# CREATE CATBOOST MODEL\ncatboost_classifier = CatBoostClassifier()\n#CREATE GRID OF PARAMETERS\ngrid = {'iterations': [10, 50, 80],\n        'learning_rate': [0.03, 0.05],\n        'depth': [4, 5, 6],\n        'l2_leaf_reg': [0.2, 0.5]}\n# Define the parameter grid\ngrid_search = GridSearchCV(estimator=catboost_classifier, param_grid=grid, cv=3, n_jobs=-1)\n\n# Fit the grid search to your data\ngrid_search.fit(x_train, y_train)  # Replace X and y with your feature and label data\n# Get the best parameters and estimator\n# best: {'depth': 6, 'iterations': 80, 'l2_leaf_reg': 0.2, 'learning_rate': 0.05}\nbest_params = grid_search.best_params_\nbest_params['iterations'] = 4000\n# best_estimator = grid_search.best_estimator_\n# y_pred = best_estimator.predict(x_test)\nclf = CatBoostClassifier(**best_params)\nclf.fit(train_dataset, eval_set=val_dataset, plot=True)\nbow_val_acc = clf.eval_metrics(val_dataset, metrics=[\"Accuracy\"])\n\n# Calculate accuracy\ny_pred = clf.predict(test_dataset)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Calculate precision\nprecision = precision_score(y_test, y_pred, average=None)\nrecall = recall_score(y_test, y_pred, average=None)\nf1 = f1_score(y_test, y_pred, average=None)\nconfusion = confusion_matrix(y_test, y_pred)\n\nperformance_metrics = {\n    \"val_accuracy\": bow_val_acc,\n    \"accuracy\": accuracy,\n    \"precision\": precision,\n    \"recall\": recall,\n    \"f1\": f1,\n    \"confusion_matrix\": confusion\n}\n# save val acc\nfile_path = 'catboost_info/bow_res.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(performance_metrics, file)\n\nNow run catboost on IDF data.\n\nx = idf()\ny = pd.read_csv(\"y.csv\").iloc[:,1:]\n# split data train,val,test\n# Split the data into training (70%), validation (15%), and test (15%) sets\nx_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42)\n\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n# BRING DATA: SPLIT INTO TRAIN, VALIDATION AND TEST\ntrain_dataset = cb.Pool(x_train, y_train) \nval_dataset = cb.Pool(x_val, y_val) \ntest_dataset = cb.Pool(x_test, y_test)\n# CREATE CATBOOST MODEL\ncatboost_classifier = CatBoostClassifier()\n# CREATE GRID OF PARAMETERS\ngrid = {'iterations': [80],\n        'learning_rate': [0.03, 0.05],\n        'depth': [4, 5, 6],\n        'l2_leaf_reg': [0.2, 0.5]}\n# Define the parameter grid\ngrid_search = GridSearchCV(estimator=catboost_classifier, param_grid=grid, cv=3, n_jobs=-1)\n# Fit the grid search to your data\ngrid_search.fit(x_train, y_train)  # Replace X and y with your feature and label data\n\n# Get the best parameters and estimator\nbest_params = grid_search.best_params_\nbest_params['iterations'] = 4000\n# best_estimator = grid_search.best_estimator_\n# y_pred = best_estimator.predict(x_test)\nclf = CatBoostClassifier(**best_params)\nclf.fit(train_dataset, eval_set=val_dataset, plot=True)\nidf_val_acc = clf.eval_metrics(val_dataset, metrics=[\"Accuracy\"])\n\n# Calculate accuracy\ny_pred = clf.predict(test_dataset)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Calculate precision\nprecision = precision_score(y_test, y_pred, average=None)\nrecall = recall_score(y_test, y_pred, average=None)\nf1 = f1_score(y_test, y_pred, average=None)\nconfusion = confusion_matrix(y_test, y_pred)\n\nperformance_metrics = {\n    \"val_accuracy\": idf_val_acc,\n    \"accuracy\": accuracy,\n    \"precision\": precision,\n    \"recall\": recall,\n    \"f1\": f1,\n    \"confusion_matrix\": confusion\n}\n# save val acc\nfile_path = 'catboost_info/idf_res.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(performance_metrics, file)\n\nNow we do catboost on word embeddings.\n\n# split data train,val,test\nx_train, x_val, x_test, y_train, y_val, y_test = embeddings_data_prep()\n\n# BRING DATA: SPLIT INTO TRAIN, VALIDATION AND TEST\ntrain_dataset = cb.Pool(x_train, y_train) \nval_dataset = cb.Pool(x_val, y_val) \ntest_dataset = cb.Pool(x_test, y_test)\n# CREATE CATBOOST MODEL\ncatboost_classifier = CatBoostClassifier()\n# CREATE GRID OF PARAMETERS\ngrid = {'iterations': [80],\n        'learning_rate': [0.03, 0.05],\n        'depth': [4, 5, 6],\n        'l2_leaf_reg': [0.2, 0.5]}\n# Define the parameter grid\ngrid_search = GridSearchCV(estimator=catboost_classifier, param_grid=grid, cv=3, n_jobs=-1)\n# Fit the grid search to your data\ngrid_search.fit(x_train, y_train)  # Replace X and y with your feature and label data\n\n#Get the best parameters and estimator\nbest_params = grid_search.best_params_\nbest_params['iterations'] = 4000\n# best_estimator = grid_search.best_estimator_\n# y_pred = best_estimator.predict(x_test)\nclf = CatBoostClassifier(**best_params)\nclf.fit(train_dataset, eval_set=val_dataset, plot=True)\nembed_val_acc = clf.eval_metrics(val_dataset, metrics=[\"Accuracy\"])\n\n# Calculate accuracy\ny_pred = clf.predict(test_dataset)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Calculate precision\nprecision = precision_score(y_test, y_pred, average=None)\nrecall = recall_score(y_test, y_pred, average=None)\nf1 = f1_score(y_test, y_pred, average=None)\nconfusion = confusion_matrix(y_test, y_pred)\n\nperformance_metrics = {\n    \"val_accuracy\": embed_val_acc,\n    \"accuracy\": accuracy,\n    \"precision\": precision,\n    \"recall\": recall,\n    \"f1\": f1,\n    \"confusion_matrix\": confusion\n}\n# save val acc\nfile_path = 'catboost_info/embed_res.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(performance_metrics, file)\n\nPlot validation accuracy.\n\nwith open('catboost_info/bow_res.pkl', 'rb') as file:\n    cbbow_res = pickle.load(file)\n\nwith open('catboost_info/idf_res.pkl', 'rb') as file:\n    cbidf_res = pickle.load(file)\n\nwith open('catboost_info/embed_res.pkl', 'rb') as file:\n    cbembed_res = pickle.load(file)\n\n\ncbbow_val = bow_res[\"val_accuracy\"][\"Accuracy\"]\ncbidf_val = idf_res[\"val_accuracy\"][\"Accuracy\"]\ncbembed_val = embed_res[\"val_accuracy\"][\"Accuracy\"]\ncb_bow = range(4000)\ncb_idf = range(3995)\ncb_embed = range(130)\n\n\nfig, ax = plt.subplots()\n\n# Plot the validation values for each experiment\nax.plot(cb_bow, cbbow_val, label='BoW')#, marker='o')\nax.plot(cb_idf, cbidf_val, label='IDF')#, marker='s')\nax.plot(cb_embed, cbembed_val, label='Embeddings')#, marker='^')\n\n# Set labels and title\nax.set_xlabel('Epochs')\nax.set_ylabel('Validation Accuracy')\n#ax.set_title('Validation Accuracy Comparison')\nax.legend()\nplt.grid(True)\nplt.savefig('catboost_info/cb_val_acc.png')#, bbox_inches='tight')\nplt.show()\n\n\n\nNaive Bayes\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical \nimport numpy as np\nimport pandas as pd\n# from bow import bow_x, bow_y\n# from tf_idf import idf\n# from embedding import embeddings_data_prep\n\nNB model for BoW data first.\n\nx = bow_x()\ny = bow_y()\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n# Split your data into training and validation sets\nX_train = X_train.values\nX_test = X_test.values\n\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the labels to integer labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Convert to one-hot encoded vector\ny_train = to_categorical(y_train_encoded)\ny_test = to_categorical(y_test_encoded)\n\nMake a function to train NB model on custom data.\n\ndef naive_bayes(X_train, y_train, X_test, y_test):\n    # Define a grid of hyperparameter values to search through\n    param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n    # Create a Multinomial Naive Bayes classifier\n    nb_classifier = MultinomialNB()\n    # Perform a grid search to find the best hyperparameter ('alpha')\n    grid_search = GridSearchCV(nb_classifier, param_grid, cv=5, scoring='accuracy')\n    # Fit the grid search to the training data\n    grid_search.fit(X_train, np.argmax(y_train, axis=1))\n    # Get the best Naive Bayes classifier based on the grid search results\n    best_nb_classifier = grid_search.best_estimator_\n    # Make predictions on the test set using the best classifier\n    y_pred_nb = best_nb_classifier.predict(X_test)\n    # Extract the true labels from one-hot encoded 'y_test'\n    y_true = np.argmax(y_test, axis=1)\n    # Calculate the accuracy of the classifier on the test set\n    accuracy = accuracy_score(y_true, y_pred_nb)\n    # Store the results including accuracy, best alpha, and the best classifier\n    results = {\n        'accuracy': accuracy,\n        'best_alpha': grid_search.best_params_['alpha'],\n        'fitted_model': best_nb_classifier\n    }\n    # Create a DataFrame to store results for reporting\n    results_df = pd.DataFrame([{\n        'best_alpha': results['best_alpha'],\n        'train_accuracy': accuracy_score(np.argmax(y_train, axis=1), best_nb_classifier.predict(X_train)),\n        'test_accuracy': accuracy\n    }])\n    \n    # Return the results, results DataFrame, and the best classifier\n    return results, results_df, best_nb_classifier\n\nopt = naive_bayes(X_test=X_test,X_train=X_train,y_test=y_test, y_train=y_train)\n\n# train the best model and output\nmod = MultinomialNB()\nparam = {'alpha': [1.0]}\ngrid = GridSearchCV(mod, param, cv=5, scoring='accuracy')\ngrid.fit(X_train, np.argmax(y_train, axis=1))\nbest_mod = grid.best_estimator_\ny_pred = best_mod.predict(X_test)\ny_true = np.argmax(y_test, axis=1)\nreport = classification_report(y_true, y_pred)\n\nNow do it for TF-IDF data.\n\nx = idf()\ny = bow_y()\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n# Split your data into training and validation sets\nX_train = X_train.values\nX_test = X_test.values\n\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the labels to integer labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Convert to one-hot encoded vector\ny_train = to_categorical(y_train_encoded)\ny_test = to_categorical(y_test_encoded)\n\nopt = naive_bayes(X_test=X_test,X_train=X_train,y_test=y_test, y_train=y_train)\n\n# train the best model and output\nmod = MultinomialNB()\nparam = {'alpha': [0.1]}\ngrid = GridSearchCV(mod, param, cv=5, scoring='accuracy')\ngrid.fit(X_train, np.argmax(y_train, axis=1))\nbest_mod = grid.best_estimator_\ny_pred = best_mod.predict(X_test)\ny_true = np.argmax(y_test, axis=1)\nreport = classification_report(y_true, y_pred)\n\nNow train the NB model with embeddings.\n\nX_train, X_val, X_test, y_train, y_val, y_test = embeddings_data_prep()\n\nX_test = np.concatenate((X_val, X_test), axis=0)\ny_test = np.concatenate((y_val, y_test), axis=0)\n\n# encode the y values\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the labels to integer labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Convert to one-hot encoded vector\ny_train = to_categorical(y_train_encoded)\ny_test = to_categorical(y_test_encoded)\n\nopt = naive_bayes(X_test=X_test,X_train=X_train,y_test=y_test, y_train=y_train)\n\n# train the best model and output\nmod = MultinomialNB()\nparam = {'alpha': [10]}\ngrid = GridSearchCV(mod, param, cv=5, scoring='accuracy')\ngrid.fit(X_train, np.argmax(y_train, axis=1))\nbest_mod = grid.best_estimator_\ny_pred = best_mod.predict(X_test)\ny_true = np.argmax(y_test, axis=1)\nreport = classification_report(y_true, y_pred)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my first assignment!",
    "section": "",
    "text": "Welcome!"
  }
]