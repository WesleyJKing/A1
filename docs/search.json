[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my first assignment!",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "allcode.html",
    "href": "allcode.html",
    "title": "Welcome to all my code",
    "section": "",
    "text": "In this section, we will perform data cleaning and prepare various datasets for analysis.\n\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nExtract names of all data files, store in files array, and extract names of all presidents and store in president_names array.\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n\n\n\n\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n\n\n\n\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "allcode.html#data-cleaning",
    "href": "allcode.html#data-cleaning",
    "title": "Welcome to all my code",
    "section": "",
    "text": "In this section, we will perform data cleaning and prepare various datasets for analysis.\n\nimport os\nimport numpy as np\nimport torch\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nExtract names of all data files, store in files array, and extract names of all presidents and store in president_names array.\n\nfolder_path = 'data'\nfiles = os.listdir(folder_path)\nfiles = [file for file in files if os.path.isfile(os.path.join(folder_path, file))]\n\nfor file in files[:3]:\n    print(file)\n\npresident_names = []\n\n# Define a regular expression pattern to match the president's name\npattern = r'_(.*?)\\.txt'\n\nfor file in files:\n    match = re.search(pattern, file)\n    if match:\n        president_name = match.group(1)\n        # remove prefix and if not there its fine already\n        president_name = president_name.replace('post_elections_', '').replace('pre_elections_', '')\n        president_names.append(president_name)\n\nMake dataset of presidents (column 1) and sentences (column 2).\n\ndf = pd.DataFrame(columns=['completion', 'prompt'])\n\n# read in file and skip the first two lines\nfor file_index in range(len(files)):\n\n    file_path = f'data/{files[file_index]}'  \n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()[2:]\n\n    # Combine the lines into a single text\n    text = ' '.join(lines)\n\n    # tokenize the text into sentences using NLTK\n    sentences = sent_tokenize(text) # remove \"\\n\" \n    # remove \"\\n\"\n    cleaned_sentences = [sentence.replace('\\n', '') for sentence in sentences]\n\n    current_president = president_names[file_index]\n    dftemp = pd.DataFrame({'completion': current_president,  'prompt': cleaned_sentences})\n    df = pd.concat([df,dftemp], axis=0)\n\n# remove stopwords function\ndef remove_stopwords(sentence):\n    words = sentence.split()\n    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n    return \" \".join(filtered_words)\n\ndf['prompt'] = df['prompt'].apply(remove_stopwords)\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.to_csv(\"data.csv\")\n\n\n\n\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n\n\n\n\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "allcode.html#bag-of-words",
    "href": "allcode.html#bag-of-words",
    "title": "Welcome to all my code",
    "section": "Bag of Words",
    "text": "Bag of Words\n\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_x():\n    # Read the data once\n    data = pd.read_csv(\"data.csv\")\n    \n    # Extract relevant columns\n    text_data = data['prompt']\n    y = data['completion']\n    \n    # Initialize a CountVectorizer for BOW representation\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\")\n    \n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # Create a DataFrame from the BOW representation\n    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return bow_df\n\n# function to return names of presidents\ndef bow_y():\n  y = names['completion']\n  return(y)\n\n\nTerm Frequency - Inverse Document Frequency\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "allcode.html#term-frequency---inverse-document-frequency",
    "href": "allcode.html#term-frequency---inverse-document-frequency",
    "title": "Welcome to all my code",
    "section": "Term Frequency - Inverse Document Frequency",
    "text": "Term Frequency - Inverse Document Frequency\n\n# Imports\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef tf_idf():\n    df = pd.read_csv(\"data.csv\").iloc[:,1:]\n    sentences = df['prompt'].tolist()\n\n    # Create a TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Fit and transform the sentences to compute TF-IDF values\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Create a new dataframe with TF-IDF values\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
  },
  {
    "objectID": "allcode.html#word-embedding",
    "href": "allcode.html#word-embedding",
    "title": "Welcome to all my code",
    "section": "Word Embedding",
    "text": "Word Embedding\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef embeddings_data_prep():\n    data = pd.read_csv(\"data.csv\").iloc[:, 1:]\n\n    # Tokenization\n    max_features = 1000\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data['prompt'])\n    sequences = tokenizer.texts_to_sequences(data['prompt'])\n\n    # Filter out empty sequences and corresponding labels\n    filtered_indices = [i for i, s in enumerate(sequences) if len(s) &gt; 0]\n    sequences = [sequences[i] for i in filtered_indices]\n    y = data['completion'].iloc[filtered_indices].values\n\n    # Splitting data into training, validation, and test sets\n    x_train, x_temp, y_train, y_temp = train_test_split(sequences, y, test_size=0.3, random_state=42)\n    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n    maxlen = 50\n    x_train_pad = pad_sequences(x_train, maxlen=maxlen)\n    x_val_pad = pad_sequences(x_val, maxlen=maxlen)\n    x_test_pad = pad_sequences(x_test, maxlen=maxlen)\n\n    return x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test\n\n\n#x_train_pad, x_val_pad, x_test_pad, y_train, y_val, y_test = embeddings_data_prep()"
  },
  {
    "objectID": "allcode.html#exploratory-data-analysis",
    "href": "allcode.html#exploratory-data-analysis",
    "title": "Welcome to all my code",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nNumber of sentences per president.\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndata = pd.read_csv(\"data.csv\").iloc[:,1:]\nsentence_counts = data['completion'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=sentence_counts.index, y=sentence_counts.values)\n\nplt.xlabel('Presidents')\nplt.ylabel('Number of Sentences')\nplt.savefig('eda_plots/sentence_counts.png', bbox_inches='tight')\nplt.show()\n\nSentence length plot.\n\ndata['sentence_length'] = data['prompt'].apply(lambda x: len(x.split()))\naverage_sentence_length = data.groupby('completion')['sentence_length'].mean().reset_index()\ndesired_order = [4, 2, 3, 1, 0, 5]\naverage_sentence_length = average_sentence_length.loc[desired_order]\n# Plot the barplot of average sentence lengths per president\nplt.figure(figsize=(10, 6))\nsns.barplot(x='completion', y='sentence_length', data=average_sentence_length)\nplt.xlabel('Presidents')\nplt.ylabel('Average Sentence Length')\nplt.savefig('eda_plots/sentence_length.png', bbox_inches='tight')\nplt.show()\n\nProduce word cloud.\n\npresidents = data['completion'].unique()\nfor president in presidents:\n    text = \" \".join(data[data['completion'] == president]['prompt'])\n    \n    # Create a WordCloud object\n    wordcloud = WordCloud(width=800, height=400, background_color='grey').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    #plt.title(f'Word Cloud for President {president}')\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "allcode.html#models",
    "href": "allcode.html#models",
    "title": "Welcome to all my code",
    "section": "Models",
    "text": "Models\n\nNeural Network\n\n# Import required libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport sklearn\nimport pickle\nfrom bow import bow_x, bow_y\n\n# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Keras specific\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nx = bow_x()\ny = bow_y()\ndef data_prep(x,y):\n    X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42)\n    # Split your data into training and validation sets\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    X_train = X_train.values\n    X_val = X_val.values\n    X_test = X_test.values\n\n    label_encoder = LabelEncoder()\n\n    # Fit and transform the labels to integer labels\n    y_train_encoded = label_encoder.fit_transform(y_train)\n    y_val_encoded = label_encoder.fit_transform(y_val)\n    y_test_encoded = label_encoder.fit_transform(y_test)\n\n    # Convert to one-hot encoded vector\n    y_train = to_categorical(y_train_encoded)\n    y_val = to_categorical(y_val_encoded)\n    y_test = to_categorical(y_test_encoded)\n\n    # dimensions\n    inp_dim = X_test.shape[1]\n    count_classes = y_test.shape[1]\n    return {\n        'X_train': X_train,\n        'X_val': X_val,\n        'X_test': X_test,\n        'y_train': y_train,\n        'y_val': y_val,\n        'y_test': y_test,\n        'inp_dim': inp_dim,\n        'count_classes': count_classes\n    }\ndata = data_prep(x,y)\nX_train = data['X_train']\nX_val = data['X_val']\nX_test = data['X_test']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test= data['y_test']\ninp_dim = data['inp_dim']\ncount_classes = data['count_classes']\nnum_epochs = 20\n\n# function to train and test model with specific params\ndef create_custom_model(neurons_per_layer, l2_reg_value):\n    # Create a Sequential model\n    model = Sequential()\n\n    # Add the input layer with L2 regularization\n    model.add(Dense(neurons_per_layer[0], activation='relu', input_dim=inp_dim, kernel_regularizer=l2(l2_reg_value)))\n\n    # Add hidden layers with L2 regularization\n    for num_neurons in neurons_per_layer[1:]:\n        model.add(Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_reg_value)))\n\n    # Add the output layer with L2 regularization\n    model.add(Dense(count_classes, activation='softmax', kernel_regularizer=l2(l2_reg_value)))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model with validation data\n    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n\n    # Evaluate the model on training and test data\n    scores_train = model.evaluate(X_train, y_train, verbose=1)\n    scores_test = model.evaluate(X_test, y_test, verbose=0)\n    \n\n    # print('Accuracy on training data: {:.2f}%\\nError on training data: {:.2f}'.format(scores_train[1] * 100, (1 - scores_train[1]) * 100))\n    # print('Accuracy on test data: {:.2f}%\\nError on test data: {:.2f}'.format(scores_test[1] * 100, (1 - scores_test[1]) * 100))\n\n    # Access validation scores from the history object\n    val_loss = history.history['val_loss']\n    val_accuracy = history.history['val_accuracy']\n\n    y_pred = model.predict(X_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels\n\n    # Convert one-hot encoded ground truth labels to class labels\n    y_true_classes = np.argmax(y_test, axis=1)\n\n    # Calculate accuracy, precision, recall, and F1 score\n    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n\n    # print('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n    # print('Precision on test data: {:.2f}'.format(precision))\n    # print('Recall on test data: {:.2f}'.format(recall))\n    # print('F1 score on test data: {:.2f}'.format(f1))\n\n    return {\n        'val_loss': val_loss,\n        'val_accuracy': val_accuracy,\n        'train_loss': history.history['loss'],\n        'train_accuracy': history.history['accuracy'],\n        'test_loss': scores_test[0],\n        'test_accuracy': scores_test[1],\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n        }\n\nCreate 3 model architectures, and implement L2 regularization on best performing one.\n\nbow_results_2 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\nbow_results_3 = create_custom_model(neurons_per_layer=[500,200,100], l2_reg_value=0)\nbow_results_4 = create_custom_model(neurons_per_layer=[800,400,100,50], l2_reg_value=0)\n\n# bow_results_2[\"test_accuracy\"] #0.5952890515327454\n# bow_results_3[\"test_accuracy\"] #0.5852962136268616\n# bow_results_4[\"test_accuracy\"] #0.6038544178009033\n\nNow regularize the 2 layer Neural Network.\n\nbow_results_2_001 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.01)\n\nbow_results_2_005 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.05)\n\nbow_results_2_01 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.1)\n\n# bow_results_2_001[\"test_accuracy\"] #0.5952890515327454\n# bow_results_2_005[\"test_accuracy\"] #0.5852962136268616\n# bow_results_2_01[\"test_accuracy\"] #0.6038544178009033\n\nbow_results_2_0 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\n\nfile_path = 'nn_res/bow_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(bow_results_2_0, file)\n\nNow do the same proces but for Term Frequency Inverse Document Frequency data set.\n\nfrom tf_idf import idf\n# import idf data\nx = idf()\ndata = data_prep(x,y)\nX_train = data['X_train']\nX_val = data['X_val']\nX_test = data['X_test']\ny_train = data['y_train']\ny_val = data['y_val']\ny_test= data['y_test']\ninp_dim = data['inp_dim']\ncount_classes = data['count_classes']\n\n# idf_results_2 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\n# idf_results_3 = create_custom_model(neurons_per_layer=[500,200,100], l2_reg_value=0)\n# idf_results_4 = create_custom_model(neurons_per_layer=[800,400,100,50], l2_reg_value=0)\n\n# idf_results_2[\"test_accuracy\"] #0.600285530090332\n# idf_results_3[\"test_accuracy\"] #0.599571704864502\n# idf_results_4[\"test_accuracy\"] #0.5888651013374329\n\n\nidf_results_2_001 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.01)\n\nidf_results_2_005 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.05)\n\nidf_results_2_01 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0.1)\n\n# no reg is the best\nidf_results_2_0 = create_custom_model(neurons_per_layer=[500,200], l2_reg_value=0)\n\n\n# idf_results_2_001[\"test_accuracy\"] #0.600285530090332\n# idf_results_2_005[\"test_accuracy\"] #0.599571704864502\n# idf_results_2_01[\"test_accuracy\"] #0.5888651013374329\nidf_results_2_0[\"test_accuracy\"]\n\nfile_path = 'nn_res/idf_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(idf_results_2_0, file)\n\nAnd train networks for word embeddings.\n\nfrom embedding import embeddings_data_prep\n\nX_train, X_val, X_test, y_train, y_val, y_test = embeddings_data_prep()\n# encode the y values\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the labels to integer labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.fit_transform(y_val)\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Convert to one-hot encoded vector\ny_train = to_categorical(y_train_encoded)\ny_val = to_categorical(y_val_encoded)\ny_test = to_categorical(y_test_encoded)\n\n# dimensions\ninp_dim = X_test.shape[1]\ncount_classes = y_test.shape[1]\n\n\n# train models\nnum_epochs = 100\n# embeds_results_2 = create_custom_model(neurons_per_layer=[40,10], l2_reg_value=0)\n# embeds_results_3 = create_custom_model(neurons_per_layer=[40,30,10], l2_reg_value=0)\n# embeds_results_4 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0)\n\n# embeds_results_2[\"test_accuracy\"] #0.2942446172237396\n# embeds_results_3[\"test_accuracy\"] #0.2942446172237396\n# embeds_results_4[\"test_accuracy\"] #0.3179856240749359\n\n# now with reg\n\n# embeds_results_2_001 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.01)\n\n# embeds_results_2_005 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.05)\n\n# embeds_results_2_01 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0.1)\n\n\n# embeds_results_2_001[\"test_accuracy\"] #0.600285530090332\n# embeds_results_2_005[\"test_accuracy\"] #0.599571704864502\n# embeds_results_2_01[\"test_accuracy\"] #0.5888651013374329\nnum_epochs = 200\nembeds_results_2_0 = create_custom_model(neurons_per_layer=[40,30,20,10], l2_reg_value=0)\n\n\nfile_path = 'nn_res/embed_results_2_0.pkl'\nwith open(file_path, 'wb') as file:\n    pickle.dump(embeds_results_2_0, file)\n\nPLOT MODEL INFORMATION.\n\nwith open(\"nn_res/bow_results_2_0.pkl\", 'rb') as file:\n    bow_res = pickle.load(file)\n\nwith open(\"nn_res/idf_results_2_0.pkl\", 'rb') as file:\n    idf_res = pickle.load(file)\n\nwith open(\"nn_res/embed_results_2_0.pkl\", 'rb') as file:\n    embed_res = pickle.load(file)\n\nbow_val = bow_res[\"val_accuracy\"]\nidf_val = idf_res[\"val_accuracy\"]\nembed_val = embed_res[\"val_accuracy\"][::10]\nx_bow = range(len(bow_val))\nx_idf = range(len(idf_val))\nx_embed = range(len(embed_val))\nx_ticks = list(range(20))\n\nfig, ax = plt.subplots()\nax.plot(x_bow, bow_val, label='BoW')#, marker='o')\nax.plot(x_idf, idf_val, label='IDF')#, marker='s')\nax.plot(x_embed, embed_val, label='Embeddings')#, marker='^')\n\n# Set labels and title\nax.set_xlabel('Epochs')\nax.set_ylabel('Validation Accuracy')\nax.set_title('Validation Accuracy Comparison')\nax.legend()\nax.set_xticks(range(21))\nplt.grid(True)\nplt.savefig('nn_res/nn_val_acc.png', bbox_inches='tight')\nplt.show()"
  }
]