<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Wesley King">

<title>A Quarto Website - Who said that?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">A Quarto Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./allcode.html" rel="" target="">
 <span class="menu-text">Welcome to all my code</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./writeup.html" rel="" target="" aria-current="page">
 <span class="menu-text">Who said that?</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#literature-review" id="toc-literature-review" class="nav-link" data-scroll-target="#literature-review">Literature review</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  <li><a href="#eda" id="toc-eda" class="nav-link" data-scroll-target="#eda">EDA</a></li>
  <li><a href="#modelling" id="toc-modelling" class="nav-link" data-scroll-target="#modelling">Modelling</a></li>
  <li><a href="#model-diagnostics" id="toc-model-diagnostics" class="nav-link" data-scroll-target="#model-diagnostics">Model diagnostics</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Who said that?</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Wesley King </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>This study analyses the intricate realm of authorship identification in South African presidential speeches, specifically those delivered during State of the Nation Addresses (SONA) spanning the period from 1994 to 2023. Employing a diverse array of machine learning (ML) techniques, including neural networks, gradient boosting trees, naive bayes, and the Electra Small language model, our research assesses the efficacy of each method in predicting authorship. Notably, the Feed-Forward Neural Network and Electra Small language model emerge as standout performers, excelling in their relative classification ability. However, this comparable performance between ELECTRA and the smaller ML models demonstrates the enduring importance of smaller ML models particularly in scenarios with limited computational resources. Their competitive accuracy, combined with a lower computational burden, positions the simpler ML models as pragmatic choices for real-time authorship text classification tasks.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The study of presidential speeches has been a focal point of political analysis for decades, offering insights into the rhetoric, style, and policy priorities of various administrations. With the recent rise in popularity of machine learning (ML), and specifically Natural Language Processing (NLP), a new task has emerged: text classification. This paper performs the task of classifying sentences based on the South African presidents who said them. The aim is to compare the performance of large, transformer-based language models with smaller, more efficient models in this classification task.</p>
<p>The need for this work arises from the increasing prevalence and importance of computationally expensive transformers in fields such as NLP and computational linguistics. These models have shown remarkable performance in various tasks, including text generation, translation, question answering, and more (Naveed et al., 2023). However, their development and running costs, as well as their computational requirements, make them less accessible for many organizations (Hart et al., 2023). This creates a gap between what the scientific community currently has - powerful but expensive and resource-intensive models - and what it wants - equally effective models that are more efficient and accessible.</p>
<p>To address this need, we have conducted a comparison of a large language model (LLM) and smaller, ML models in the task of classifying sentences of South African presidents. We have explored both the theoretical underpinnings of these models and their practical performance in the classification task.</p>
<p>The remainder of this paper is structured as follows: we first provide a comprehensive background on the models used in NLP, discussing their mechanisms, strengths, and weaknesses. We then detail our methodology for comparing these models in the task of classifying presidential sentences. Following this, we present and discuss our results, offering insights into the performance of the models. Finally, we conclude with a reflection on the implications of our findings for the future development and use of language models in political analysis and beyond.</p>
</section>
<section id="literature-review" class="level2">
<h2 class="anchored" data-anchor-id="literature-review">Literature review</h2>
<p>Natural Language Processing (NLP) has been utilized extensively for diverse tasks. These tasks range from classification in healthcare [(Ong et al., 2010), (Fujita et al., 2012), (Ravindranath et al., 2017), (McKnight, 2012), (Wong &amp; Akiyama, 2013), (Gerdes &amp; Hardahl, 2013)], to predicting financial trends [(Wen et al., 2019), (Wu et al., 2012), (Al-Rubaiee et al., 2015), (Vijayan and Potey, 2016), (Nassirtoussi et al., 2015), (Nikfarjam et al., 2010)], to applications in corporate finance [(Guo et al., 2016), (Shahi et al., 2014), (Holton, 2009), (Chan and Franklin, 2011), (Humpherys et al., 2011), (Loughran and McDonald, 2011)] and the techniques and methodologies employed vary based on the specifics of the problem.</p>
<p>For example (Dogra et al., 2022) focusses on exploring cutting-edge learning models implemented on a language dataset. Amongst the models investigated are Naive Bayes models. Through a comprehensive review of this algorithm, the authors provide valuable insights into the nuances of text classification, shedding light on the strengths and limitations of the Naive Bayes method. (Dogra et al., 2022) also analyses data formatting techniques such as bag-of-words (BoW), Term Frequency - Inverse Document Frequency (TF-IDF), and word embeddings. BoW is most commonly used in document transformation, however suffers from high dimensionality, resulting in slow compute time. TF-IDF overcomes some weaknesses of BoW, as the text is represented with weighted frequencies, however still suffers from high dimensionality which slows a model’s rate of learning. Word embeddings allow for a reduced dimensional dataset to be analysed by representing each sentence by a vector of fixed size, where each unique word is a unique integer, dependent on the word and its context.</p>
<p>In 2019, (Devlin et al., 2019) developed the language model known as BERT which borrowed the Encoder from the previously created Transformer architecture (Vaswani et al., 2017). Although upon its release BERT set new standards for numerous NLP related performance metrics, it does have its shortcomings. Due to the nature of BERT’s training approach being one of masking a fraction of the input tokens and asking the model to predict the token being masked, it doesn’t efficiently utilize all of it’s data, with predictions being made on only 15% of the tokens in each batch (Devlin et al., 2019). (Clark et al., 2020) developed ELECTRA to handle just this problem. Instead of training a model to predict the original identity of a corrupted token, ELECTRA is a discriminator trained to predict whether each token in the corrupted input was replaced by a generator sample or not. This approach to training is shown to be more efficient than the Masked Language Model approach to training because the task is performed on all input tokens, as oppose to just a select few, and it resulted in ELECTRA achieving a better General Language Understanding Evaluation (GLUE) score than BERT.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="data-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h3>
<p>To get the data into a format compliant with the chosen language models, numerous cleaning operations had to be performed. Firstly, to identify the presidents associated with each speech, a regular expression pattern was defined. This pattern extracted the president’s name from the document names, which are named after the president which delivered the speech contained in the text document. A tabular data structure was then initialized with two columns: ‘Completion’ and ‘Prompt’. This DataFrame serves as the container for organizing the processed data. For each data file, the content is read in while disregarding the initial two lines, which contains the date and an empty line. The content is stored as a list of lines. These lines are then concatenated into a single text document, before being stripped of symbols, such as ‘’. The full text is then tokenized into sentences, with the name of the president inserted into the ‘completion’ column next to each sentence. Stop words where then removed too, which reduces the amount of unnessecary data. This resulted in a dataset with 9337 sentences, and served as a starting point for 3 other datasets to be formed.</p>
<section id="bag-of-words-bow" class="level4">
<h4 class="anchored" data-anchor-id="bag-of-words-bow">Bag of Words (BoW)</h4>
<p>BoW (Harris, 1954) is a data manipulating technique which has a variety of use cases, such as in Image Classification, Text Classification and Visual Scene Classification (Qader et al., 2019). In the context of visual scene classification, the BoW approach allows for clusters of local descriptors to be extracted from the images, where the order of the clusters is not important. In the domain of text classification, which is the field of this study, the BoW methodology entails counting the number of occurrences associated with each word within a given sentence, while disregarding the inherent word order and grammatical structure. The fundamental concept underlying BoW is the generation of a histogram, capturing the frequency distribution of words within textual documents or the prevalence of features within images, thereby creating a representative profile for the respective sentence or image which attempt to capture the key aspects. Notably, the BoW technique is characterized by its computational simplicity, rendering it more accessible and conceptually straightforward than many alternative classification methodologies (Qader et al., 2019). Consequently, BoW-based systems have demonstrated the potential to achieve improved performance metrics on widely accepted benchmarks for evaluating text and image classification algorithms.</p>
<p>In order to transform our dataframe consisting of a ‘completion’ and ‘prompt’ column into a BoW dataset, 3 steps need to be followed. Firstly, Text Tokenization needs to occur, which involves the segmentation of input text into individual words or terms, commonly referred to as tokens. Then a vocabulary has to be formed which is a dictionary comprising of all the distinct words (tokens) identified within the text corpus. Each of these unique words is associated with a specific integer index. Finally, Token Frequency Counting must occur. At this stage, the number of occurrences of each word (token) within every sentence present in the text corpus must be counted. The outcome is a matrix where each row corresponds to a sentence, and each column corresponds to a unique word from the vocabulary. The entries in this matrix reflect the frequency (count) of each word within each sentence. Thus, if the matrix is called <span class="math inline">\(D\)</span>, entry <span class="math inline">\(D_{i,j}\)</span> is the number of times word <span class="math inline">\(j\)</span> appeared in sentence <span class="math inline">\(i\)</span>. These 3 steps are easily performed using <code>sklearn</code> (Buitinck et al., 2013) and more specifically the <code>CountVectorizer()</code> and <code>fit_transform()</code> functions.</p>
</section>
<section id="term-frequency---inverse-document-frequency-tf-idf" class="level4">
<h4 class="anchored" data-anchor-id="term-frequency---inverse-document-frequency-tf-idf">Term Frequency - Inverse Document Frequency (TF-IDF)</h4>
<p>TF-IDF (Luhn, 1957) is a numerical statistic used in text mining and information retrieval in order to reflect how important a word is in a document in a collection or corpus based on two factors. The TF-IDF value increases proportionally to the number of times a word appears in the document and it decreases based on the number of documents in the corpus that contain the word. This helps to offset the frequency bias which would declare commonly used words as more important than smaller ones. TF-IDF is thus composed of two terms, Term Frequency (TF) and Inverse Document Frequency (IDF). TF measures the frequency of a word in a document, thus if a word appears more often in a document, its TF value increases according to the following formula:</p>
<p><span class="math display">\[
\text{TF(t)} = \frac{\text{Number of times term t appears in a document}}{\text{Total number of terms in the document}}
\]</span></p>
<p>IDF measures the importance of a word across a set of sentences, in our case, but can be documents in other cases. If a word appears in many sentences, it’s not a unique identifier of that sentence, therefore its IDF value decreases. The IDF of a word <span class="math inline">\(t\)</span> can be calculated as follows:</p>
<p><span class="math display">\[
\text{IDF(t)} = ln \left( \frac{\text{Total number of sentences}}{\text{Number of sentences with term t in it}}\right )
\]</span></p>
<p>The TF-IDF value of a word in a particular sentence is the product of its TF and IDF values, thus: <span class="math display">\[
\text{TF-IDF(t)} = \text{TF(t)} \times \text{IDF(t)}
\]</span></p>
<p>This value is thus larger for words that are more unique to a sentence, and lower for words that are common across many sentences.</p>
<p>A key difference between the BoW model and TF-IDF is that BoW can be seen to suffer from frequency bias as it does not consider the uniqueness of words to a sentence, but rather places importance based on frequency. Thus common words in our case like ‘Government’ and ‘Country’ will be valued highly simply because they are used often. However, in the TF-IDF approach these words will have low scores as they appear in many documents, and therefore are not unique or informative.</p>
<p>In order to convert our dataset of a ‘completion’ and ‘prompt’ column into a TF-IDF dataset, similar steps shown above must be followed. Again, tokenization must first occur, where input text is decomposed into discrete words (tokens). This process partitions the text into individual words or terms, making it amenable to further work. Following tokenization, a dictionary containing all distinct words (tokens) encountered within the text corpus must be built. Each unique token within this vocabulary is assigned a unique integer index for identification. Then TF-IDF Transformation occurs. This diverges from the conventional token counting approach, as the TF, IDF, and then TF-IDF values for each token within every sentence is calculated. The outcome of this process is a TF-IDF matrix. Similarly to the BoW matrix, each row within the matrix corresponds to an individual sentence, while each column corresponds to a unique token derived from the vocabulary. The matrix entries however signify the computed TF-IDF values for each token within each document, encapsulating the importance and discriminatory potential of each term. Once again, this process is largely simplified by utilizing the <code>sklearn</code> package and more specifically the <code>TfidfVectorizer()</code> and <code>fit_transform()</code> functions.</p>
</section>
<section id="word-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="word-embeddings">Word Embeddings</h4>
<p>State-of-the-art word embeddings are a vector representation that can allow words with similar meaning to have a similar representation. They are a distributed, numeric representation of text that is perhaps one of the key reasons for the impressive performance of deep learning methods on challenging NLP problems (Mikolov et al., 2013). Trained Word embedding networks map words in a vocabulary into a vector space, where the position of each word is learned from text data and is based on the context in which it is used. Thus typically, the position of a word within the learned vector space is referred to as its embedding. Word embeddings differ from BoW or TF-IDF datasets in several key ways. Firstly, word embeddings can capture much more complex constructs, like the semantic meaning of words. Words are thus defined by the surrounding words, which encourages the model to learn context. BoW and TF-IDF on the other hand disregard word order and context. Word embeddings can also have much lower dimensionality, which can reduce the compute time far greater than is the case with BoW and TF-IDF.</p>
<p>However, for this study, a simple frequency based word embedding system is used which gives values to words based soley on the frequency of their occurence. In order to transform our dataset of a ‘completion’ and ‘prompt’ column into a word embedded dataset, 4 steps need to be followed. Firstly, an upper bound of 1000 unique words is set to restrict the number of words used in the embedding. This is done to increase efficiency. Each unique word in the text corpus needs to then be ranked in descending order of frequency, thus with the most frequently used word given a value of 1, and the second most frequently used word given a value of 2, etc. Then each sentence in the dataset should be iterated through, converting each word to the corresponding number in the frequency based ranking, while removing words entirely if they do not appear within the top 1000 most frequently used words. Finally, to ensure each sentence is embedded as a vector of the equal length, an upper bound of 50 was chosen such that vectors of length less than 50 were padded with zeros, and vectors of length over 50 were truncated to the first 50 integers.</p>
</section>
</section>
<section id="eda" class="level3">
<h3 class="anchored" data-anchor-id="eda">EDA</h3>
<p>The dataset is comprised of 36 speeches made by South African presidents Zuma, Mbeki, Ramaphosa, Mandela, Motlanthe, and de Klerk, from the years 1994 to 2023. The barplot below displays the number of sentences spoken by each president. Significant variation in the number of sentences associated with each president can be seen. Notably, Jacob Zuma had the highest number of sentences attributed to him, totaling 2,629 sentences. Thabo Mbeki followed closely with 2,397 sentences, while Cyril Ramaphosa had 2,279 sentences. Nelson Mandela had 1,671 sentences in the dataset. Motlanthe and F.W. de Klerk had relatively fewer sentences, with 264 and 97 sentences, respectively. From this we can see that we expect the models to struggle to classify Motlanthe and de Klerk sentences correctly, since their respective datasets are so small.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="eda_plots/sentence_counts.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figure 1: Number of sentences per president</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="eda_plots/sentence_length.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figure 2: Average sentence length per president</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Figure 2 shows the varying sentence lengths among presidents, with President Mbeki having the longest average sentence length at approximately 15 words per sentence, and President de Klerk having the shortest at about 10 words per sentence. This variability may indicate differences in communication styles, speech contexts, or historical periods.</p>
<p>The word clouds below shows the most frequently used words by each president. Interestingly, de Klerk is the only president who regularly used the words “constitution” and “constitutional”, which could be due to him being involved in the negotiation of the new constitution in South Africa. Other commonly used words which further suggest this narrative include “right”, “amendment” and “Freedom Alliance”. “South Africa”, “Goverment” and “country” are some of the other more popular words used by the presidents, as can be seen in Mandela’s wordcloud. Interestingly, before removing the stopwords, “will” was a very popular word used by the presidents which highlights the alure of promises about the future often made by leaders.</p>
<div class="quarto-layout-panel" data-caption="Caption for the Panel Figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="eda_plots/deklerk_wordcloud.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 3: Word cloud for de Klerk</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="eda_plots/mandela_wordcloud.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 4: Word cloud for Mandela</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>With the comprehensive description and analysis of the data completed, we can now delve into the intricate details of the models developed for this classification task.</p>
</section>
<section id="modelling" class="level3">
<h3 class="anchored" data-anchor-id="modelling">Modelling</h3>
<section id="feed-forward-neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="feed-forward-neural-networks">Feed-forward Neural Networks</h4>
<p>Feed-forward Neural networks are a subset of machine learning algorithms modeled after the human brain (Lim &amp; Sung, 1970). They are composed of interconnected nodes or “neurons” that process information using a system of weights and bias values. These weights and bias values are adjusted during the training process to minimize the difference between the network’s predicted output and the actual observed value. This updating of weights occurs through a process known as backpropagation. The advantage of neural networks is their ability to identify complex patterns and relationships in data, making them especially useful for tasks such as image recognition, natural language processing, and predictive modeling.</p>
<p>To optimize the performance of the Feed-forward Neural Network, a systematic exploration was undertaken. This involved conducting a grid search, wherein various configurations were evaluated. Specifically, we focused on 3 critical aspects of the network architecture: the number of hidden layers, the number of neurons within each layer, and the L2-regularization value applied. Our initial exploration encompassed three distinct network architectures. First, a model with two hidden layers was examined, followed by one with three hidden layers and finally a model with four hidden layers. The selection of the ultimate network architecture was based on the examination of the validation accuracy. Having determined the optimal architecture, we further refined our model through the application of L2-regularization. In particular, we applied a range of L2-regularization values from 0.01 to 0.1, with the intent of identifying the value that yielded the highest validation accuracy in the selected model. This approach was replicated for each of the datasets, namely BoW, TF-IDF, and word embeddings.</p>
</section>
<section id="boosted-trees" class="level4">
<h4 class="anchored" data-anchor-id="boosted-trees">Boosted Trees</h4>
<p>Boosted trees are an ensemble of, generally, weak learners, which are sequentially trained on the errors made by their predecessors. This iterative process involves assigning weights to training data points, emphasizing the samples that are consistently misclassified. The subsequent trees are then tailored to rectify these misclassifications, incrementally improving predictive accuracy over a large number of learners. CatBoost, as introduced by (Prokhorenkova et al., 2019), distinguishes itself from the other gradient boosting algorithms such as XGBoost and LightGBM through its unique approach to tree construction. Unlike its counterparts, CatBoost constructs balanced trees that exhibit a symmetrical structure. This implies that the feature-split pair which minimizes the loss function is uniformly selected and applied to all nodes within the same level of the tree.</p>
<p>In the pursuit of optimizing the hyperparameters of the CatBoost algorithm, a grid search was conducted. The grid search was performed across a comprehensive parameter space including the parameters <code>iterations</code>, <code>learning_rate</code>, <code>depth</code>, and <code>l2_leaf_reg</code>. <code>iterations</code> determines the number of boosting iterations during the training process. <code>learning_rate</code> governs the step size at each iteration while moving towards a minimum of the loss function, while <code>depth</code> defines the maximum depth of the decision tree. Finally, <code>l2_leaf_reg</code> contributes to controlling overfitting by penalizing the magnitude of the weights. For each combination of these parameters, the CatBoost algorithm was trained and validated, and the model’s performance was assessed through the accuracy of the model on the validation set. The overarching objective was to identify the hyperparameter configuration that yielded the optimal model performance, as indicated by the validation accuracy.</p>
</section>
<section id="naive-bayes" class="level4">
<h4 class="anchored" data-anchor-id="naive-bayes">Naive Bayes</h4>
<p>The Naive Bayes algorithm is a classification technique based on Bayes’ Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event (Dogra et al., 2022). The algorithm is labeled “naive” because it assumes that all features in a dataset are mutually independent. Despite this oversimplification, Naive Bayes can be remarkably accurate, fast, and efficient, particularly with large datasets and it is widely used in text classification.</p>
<p>In order to optimise the Naive Bayes algorithm, the single <code>alpha</code> parameter was chosen such that the validation accuracy was once again maximised. <code>alpha</code> is instrumental in preventing zero proabilities occuring with words that are not found in all datasets, thus <code>alpha</code> being set to zero can greatly reduce the performance of the model. <code>alpha</code> is generally set to 1, also known as Laplace smoothing, however optimal values depend on the dataset.</p>
</section>
<section id="electra" class="level4">
<h4 class="anchored" data-anchor-id="electra">ELECTRA</h4>
<p>The ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)(Clark et al., 2020) model introduces a novel approach to pre-training in the domain of NLP. While traditional pre-training methods like BERT rely on Masked Language Modeling (MLM), where input tokens are partially obscured by replacing them with a special token ([MASK]) and the model is trained to predict the original tokens, ELECTRA takes a more data-efficient route. ELECTRA replaces select tokens in the input text with plausible alternatives drawn from a generator network. Thus rather than training a model to predict the original identities of the corrupted tokens, ELECTRA trains a discriminative model, like the one found in a Generative Adverserial Network (GAN) (Goodfellow et al., 2014). The Discriminative model’s task is thus to determine whether each token in the corrupted input was replaced by a sample generated from the network or if it is the original, unchanged token.</p>
</section>
</section>
<section id="model-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="model-diagnostics">Model diagnostics</h3>
<p>Numerous model evaluation scores were consulted during the modelling process which allowed for the relative and absolute competence of the models to be seen. Amongst the metrics investigated, is the training accuracy, which shows the ability of the model to learn from the training set, however cannot be taken as absolute model performance. For this, the out-of-sample, test set accuracy, which measures the proportion of sentences correctly attriuted to a president, must be consulted. This provides a stong indicication for the models performance as it is performed on unseen data. Precision, which measures the proportion of true positive predictions for each class among all positive predictions for that class was also consulted. This allows for a more indepth understanding of how the model performs on each class.</p>
<p><span class="math display">\[
\text{Precision}= \frac{\text{True Positives for Class}}{\text{True Positives for Class} + \text{False Positives for Class}}
\]</span></p>
<p>Another class specific metric is recall, which measures the proportion of true positive predictions for each class among all actual instances of that class.</p>
<p><span class="math display">\[
\text{Recall}= \frac{\text{True Positives for Class}}{\text{True Positives for Class} + \text{False Negatives for Class}}
\]</span></p>
<p>The final metric consulted is the F1-score for each class and is the harmonic mean of precision and recall for that class. It balances precision and recall and since our classes are imbalanced, it is a suitable, all-encompassing metric for model performance (Seo et al., 2021).</p>
<p><span class="math display">\[
\text{F1-Score}= 2\times \frac{\text{Preicision}\times \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span></p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>The table below illustrates the test set accuracy for various machine learning models across the different datasets. Notably, we observe variations in accuracy across models and data sources, which highlight the influence of both the model architecture and the data representation technique. The highest test set accuracy is achieved with the Neural Network on the TF-IDF, with a score of 59.7%, while the Naive Bayes model performs second best also on the TF-IDF dataset with a score of 59%. The Boosted Tree performs best on the Frequency embeddings with a score of 34.8%.</p>
<table class="table">
<caption>Table 1: Test set accuracy for all models</caption>
<colgroup>
<col style="width: 29%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>BoW</th>
<th>TF-IDF</th>
<th>Frequency Embeddings</th>
<th>ELECTRA Embeddings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Net</td>
<td>0.567</td>
<td>0.597</td>
<td>0.343</td>
<td></td>
</tr>
<tr class="even">
<td>Boosted Tree</td>
<td>0.549</td>
<td>0.555</td>
<td>0.348</td>
<td></td>
</tr>
<tr class="odd">
<td>Naive Bayes</td>
<td>0.580</td>
<td>0.590</td>
<td>0.300</td>
<td></td>
</tr>
<tr class="even">
<td>ELECTRA</td>
<td></td>
<td></td>
<td></td>
<td>0.580</td>
</tr>
</tbody>
</table>
<p>The table below displays F1-scores for all models, across the datasets, and for each category. The Naive Bayes model achieves the highest F1-score with a value of 0.65 for both Ramaphosa and Zuma, both while using the TF-IDF dataset. Several zero F1-scores are also present for the scarce categories of Motlanthe and deKlerk, with an F1-score of 0 being achieved by both the Neural Net and the CatBoost algorithm when the word embeddings dataset was used, as well as for the ELECTRA model.</p>
<table class="table">
<caption>Table 2: Test set F1-scores for all models</caption>
<colgroup>
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 7%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Model Type</th>
<th>Dataset</th>
<th>Motlanthe</th>
<th>Mandela</th>
<th>Mbeki</th>
<th>Ramaphosa</th>
<th>Zuma</th>
<th>deKlerk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Neural Net</strong></td>
<td>BoW</td>
<td>0.098</td>
<td>0.442</td>
<td>0.551</td>
<td>0.606</td>
<td>0.612</td>
<td>0.615</td>
</tr>
<tr class="even">
<td></td>
<td>TF-IDF</td>
<td>0.045</td>
<td>0.481</td>
<td>0.572</td>
<td>0.647</td>
<td>0.615</td>
<td>0.583</td>
</tr>
<tr class="odd">
<td></td>
<td>Frequency Embeddings</td>
<td>0.0</td>
<td>0.042</td>
<td>0.219</td>
<td>0.112</td>
<td>0.432</td>
<td>0.0</td>
</tr>
<tr class="even">
<td><strong>CatBoost</strong></td>
<td>BoW</td>
<td>0.044</td>
<td>0.465</td>
<td>0.532</td>
<td>0.583</td>
<td>0.612</td>
<td>0.381</td>
</tr>
<tr class="odd">
<td></td>
<td>TF-IDF</td>
<td>0.047</td>
<td>0.493</td>
<td>0.545</td>
<td>0.581</td>
<td>0.612</td>
<td>0.3</td>
</tr>
<tr class="even">
<td></td>
<td>Frequency Embeddings</td>
<td>0.0</td>
<td>0.008</td>
<td>0.39</td>
<td>0.235</td>
<td>0.453</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td><strong>Naive Bayes</strong></td>
<td>BoW</td>
<td>0.06</td>
<td>0.46</td>
<td>0.58</td>
<td>0.65</td>
<td>0.62</td>
<td>0.08</td>
</tr>
<tr class="even">
<td></td>
<td>TF-IDF</td>
<td>0.04</td>
<td>0.5</td>
<td>0.6</td>
<td>0.65</td>
<td>0.65</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td></td>
<td>Frequency Embeddings</td>
<td>0.02</td>
<td>0.11</td>
<td>0.31</td>
<td>0.18</td>
<td>0.47</td>
<td>0.01</td>
</tr>
<tr class="even">
<td><strong>ELECTRA</strong></td>
<td>ELECTRA Embeddings</td>
<td>0</td>
<td>0.47</td>
<td>0.54</td>
<td>0.62</td>
<td>0.61</td>
<td>0</td>
</tr>
</tbody>
</table>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>The accuracy table provides valuable insights into the performance of different machine learning models across multiple data sources. Noteworthy observations include the relatively high accuracy achieved by the Neural Network and Boosted Tree models when applied to TF-IDF data, underscoring the suitability of this data representation for these models. In contrast, the Naive Bayes model excels with BoW data, achieving the second highest test set accuracy score on this data. Additionally, the introduction of ELECTRA Embeddings used in the ELECTRA model, demonstrates competitive accuracy, suggesting the effectiveness of pre-trained embeddings in capturing complex patterns within the data. However, the fact that these fine-tuned state-of-the-art embeddings have not lead to drastically better performance in the test set highlights key insights into these models. Whether the underwhelming performance by ELECTRA is due to the lack of data (as transformers are notoriously slow to learn), or the complexity of the data, remains unknown. However, the equally competitive scores achieved by far simpler machine learning models is a good sign of the ability of simpler cost effective models to compete with an LLM such as ELECTRA.</p>
<p>In Table 2 showing F1-scores for the models, several noteworthy observations can be made. Firstly, amongst the simpler models, they all exhibit greater F1-scores over the BoW and TF-IDF datasets when compared to the word embeddings dataset, highlighting that the Data Representation certainly matters. As mentioned in the Methodology section, this is likely due to the primitive embedding system used in this section, which leads to a dataset with a far lesser dimensionality, as well as representations for words which do not represent meaning apart from relative frequency of use. As was seen in the accuracy table discussed above, the TF-IDF dataset appears to be the favoured dataset as well, followed by BoW. ELECTRA and ELECTRA Embeddings once again demonstrate competitive F1-scores. This further suggests the effectiveness of fine tuning pre-trained embeddings in capturing complex patterns within the data. The comparable scores achieved by the simpler machine learning models once again raises questions about the efficacy of LLMs such as ELECTRA when the data is not abundant. The fact that simpler machine learning models, such as “Naive Bayes” and “CatBoost,” achieve competitive F1-scores when compared to a complex language model like ELECTRA, underscores the effectiveness of simpler, cost-effective models in text classification tasks. This observation highlights the potential of simpler models to compete with LLMs like ELECTRA in the absence of extensive training data, in classification tasks.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This study analyses the performance of various machine learning models within the realm of text classification. We present a comprehensive analysis of accuracy and F1-scores, offering a deep understanding of each model’s strengths and limitations. Notably, in scenarios with limited data, simpler models such as Boosted Trees, Feed-Forward Neural Networks, and Naive Bayes demonstrate performances comparable with ELECTRA. Additionally, our exploration introduces ELECTRA Embeddings through the ELECTRA model, revealing strong accuracy and emphasizing the significance of pre-trained embeddings in capturing intricate data patterns.</p>
<p>These findings hold substantial implications for practical applications. Researchers and practitioners can leverage this analysis to make informed choices regarding model selection and data representation for specific text classification tasks. Furthermore, this study reinforces the idea that complex language models, while potent, may not consistently outperform simpler alternatives, especially when data is scarce.</p>
<p>While this research provides valuable insights, further investigations on balanced datasets with increased observations are warranted. This approach would allow us to fully assess the potential of LLMs, particularly when a large enough dataset is available, which could warrrant the large number of parameters available in an LLM.</p>
<p>Our results underscore the effectiveness of both simpler models and pre-trained embeddings, emphasizing the nuanced interplay between data representation and model performance. As the landscape of text classification tasks evolves, our research offers guidance in selecting the right models and data representations for achieving optimal, efficient outcomes.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 47.1%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="nn_res/nn_val_acc_copy.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">Figure A1: Neural Network validation accuracy</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 52.9%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="catboost_info/cb_val_acc_copy.png" class="img-fluid figure-img" width="450"></p>
<figcaption class="figure-caption">Figure A2: CatBoost validation accuracy</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="bibliography" class="level2">
<h2 class="anchored" data-anchor-id="bibliography">Bibliography</h2>
<p>AL-Rubaiee H, Qiu R, Li D (2015). Analysis of the relationship between Saudi twitter posts and the Saudi stock market. In: 2015 IEEE seventh international conference on intelligent computing and information systems (ICICIS). Available at: https://doi.org/10.1109/intelcis.2015.7397193 (Accessed: 07 October 2023).</p>
<p>Buitinck, L. et al., 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning. pp.&nbsp;108–122.</p>
<p>Chan SWK, Franklin J (2011). A text-based decision support system for financial sequence prediction. Decis Support Syst 52(1):189–198. Available at: https://www.sciencedirect.com/science/article/pii/S0167923611001230 (Accessed: 07 October 2023).</p>
<p>Clark, K. et al.&nbsp;(2020) Electra: Pre-training text encoders as discriminators rather than generators, arXiv.org. Available at: https://arxiv.org/abs/2003.10555 (Accessed: 08 October 2023).</p>
<p>Devlin, J. et al.&nbsp;(2019) Bert: Pre-training of deep bidirectional Transformers for language understanding, arXiv.org. Available at: https://arxiv.org/abs/1810.04805 (Accessed: 08 October 2023).</p>
<p>Dogra, V. et al.&nbsp;(2022) A complete process of text classification system using state-of-the-art NLP models, Computational intelligence and neuroscience. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9203176/ (Accessed: 05 October 2023).</p>
<p>Gerdes, L. and Hardahl, C. (2013) Text mining electronic health records to identify hospital adverse events, Studies in health technology and informatics. Available at: https://pubmed.ncbi.nlm.nih.gov/23920919/ (Accessed: 07 October 2023).</p>
<p>Goodfellow, I.J. et al.&nbsp;(2014) Generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1406.2661 (Accessed: 11 October 2023).</p>
<p>Guo L, Shi F, Tu J (2016). Textual analysis and machine learning: crack unstructured data in finance and accounting. J Finance Data Sci 2(3):153–170. Available at: https://www.sciencedirect.com/science/article/pii/S2405918816300496 (Accessed: 07 October 2023).</p>
<p>Hart, S. et al.&nbsp;(2023) Organizational preparedness for the use of large language models in pathology informatics, Journal of Pathology Informatics. Available at: https://www.sciencedirect.com/science/article/pii/S2153353923001529 (Accessed: 08 October 2023).</p>
<p>Harris, Z.S. (1954) Distributional structure zellig S. harris - taylor &amp; francis online. Available at: https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520 (Accessed: 09 October 2023).</p>
<p>Holton C (2009). Identifying disgruntled employee systems fraud risk through text mining: a simple solution for a multi-billion dollar problem. Decis Support Syst 46(4):853–864. Available at: https://www.sciencedirect.com/science/article/pii/S0167923608002078 (Accessed: 07 October 2023).</p>
<p>Humpherys SL, Moffitt KC, Burns MB, Burgoon JK, Felix WF (2011). Identification of fraudulent financial statements using linguistic credibility analysis. Decis Support Syst 50(3):585–594. Available at: https://www.sciencedirect.com/science/article/pii/S0167923610001338 (Accessed: 07 October 2023).</p>
<p>Loughran T, Mcdonald B (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. J Finance 66(1):35–65. Available at: https://www.sciencedirect.com/science/article/pii/S0167923610001338 (Accessed: 07 October 2023).</p>
<p>Luo, X. and AbstractText classification (TC) is an approach used for the classification of any kind of documents for the target category or out. In this paper (2021) Efficient English text classification using selected Machine Learning Techniques, Alexandria Engineering Journal. Available at: https://www.sciencedirect.com/science/article/pii/S1110016821000806 (Accessed: 05 October 2023).</p>
<p>McKnight. (2012) Semi-supervised classification of Patient Safety Event Reports, Journal of patient safety. Available at: https://pubmed.ncbi.nlm.nih.gov/22543364/ (Accessed: 07 October 2023).</p>
<p>Mikolov, T. et al.&nbsp;(2013) Efficient estimation of word representations in vector space, arXiv.org. Available at: https://arxiv.org/abs/1301.3781 (Accessed: 09 October 2023).</p>
<p>Nassirtoussi AK, Aghabozorgi S, Wah TY, Ngo DC (2015). Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment. Expert Syst Appl 42(1):306–324. https://doi.org/10.1016/j.eswa.2014.08.004 (Accessed: 07 October 2023).</p>
<p>Nikfarjam A, Emadzadeh E, Muthaiyah S (2010). Text mining approaches for stock market prediction. In: 2010 the 2nd international conference on computer and automation engineering (ICCAE). Available at: https://doi.org/10.1109/iccae.2010.5451705 (Accessed: 07 October 2023).</p>
<p>Ong, M.-S., Magrabi, F. and Coiera, E. (2010) Automated categorisation of Clinical Incident Reports Using Statistical Text Classification, BMJ Quality &amp; Safety. Available at: https://qualitysafety.bmj.com/content/19/6/e55 (Accessed: 07 October 2023).</p>
<p>Qader, W.A., Ameen, M.M. and Ahmed, B.I. (2019) An overview of bag of words; importance, implementation, applications, and Challenges. Available at: https://www.researchgate.net/publication/338511771_An_Overview_of_Bag_of_WordsImportance_Implementation_Applications_and_Challenges (Accessed: 09 October 2023).</p>
<p>Ravindranath, P. et al.&nbsp;(2017) Machine learning in Automated Classification of adverse events in clinical</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>